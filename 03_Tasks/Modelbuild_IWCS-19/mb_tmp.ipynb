{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from textwrap import fill\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file, set up paths, make project-specific imports\n",
    "config_path = os.environ.get('VISCONF')\n",
    "if not config_path:\n",
    "    # try default location, if not in environment\n",
    "    default_path_to_config = '../../clp-vision/Config/default.cfg'\n",
    "    if os.path.isfile(default_path_to_config):\n",
    "        config_path = default_path_to_config\n",
    "\n",
    "assert config_path is not None, 'You need to specify the path to the config file via environment variable VISCONF.'        \n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config.read_file(f)\n",
    "\n",
    "corpora_base = config.get('DEFAULT', 'corpora_base')\n",
    "preproc_path = config.get('DSGV-PATHS', 'preproc_path')\n",
    "dsgv_home = config.get('DSGV-PATHS', 'dsgv_home')\n",
    "\n",
    "sys.path.append(dsgv_home + '/Utils')\n",
    "from utils import icorpus_code, plot_labelled_bb, get_image_filename, query_by_id\n",
    "from utils import plot_img_cropped, plot_img_ax, invert_dict, get_a_by_b\n",
    "\n",
    "\n",
    "sys.path.append('../../Common')\n",
    "from data_utils import load_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up preprocessed DataFrames. Slow!\n",
    "# These DataFrames are the result of pre-processing the original corpus data,\n",
    "# as per dsg-vision/Preprocessing/preproc.py\n",
    "\n",
    "df_names = ['mscoco_bbdf', 'refcoco_refdf', 'refcocoplus_refdf',\n",
    "            'vgregdf', 'vgimgdf', 'vgobjdf', 'vgreldf', 'vgattdf', 'cococapdf']\n",
    "df = load_dfs(preproc_path, df_names)\n",
    "\n",
    "# a derived DF, containing only those region descriptions which I was able to resolve\n",
    "df['vgpregdf'] = df['vgregdf'][df['vgregdf']['pphrase'].notnull() & \n",
    "                               (df['vgregdf']['pphrase'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersecting visual genome and coco captions. Slow-ish.\n",
    "caption_coco_iids = list(set(df['cococapdf']['image_id'].tolist()))\n",
    "# regions for only those image for which we also have coco captions\n",
    "visgencocap_regdf = df['vgpregdf'].merge(pd.DataFrame(caption_coco_iids, columns=['coco_id']))\n",
    "# coco_image_ids for images with both caption and region\n",
    "vgcap_coco_iids = list(set(visgencocap_regdf['coco_id'].tolist()))\n",
    "# visgen_image_ids for images with both caption and region\n",
    "vgcap_vg_iids = list(set(visgencocap_regdf['image_id'].tolist()))\n",
    "\n",
    "# map coco_ids to visgen_ids, and back\n",
    "coco2vg = dict(visgencocap_regdf[['coco_id', 'image_id']].values)\n",
    "vg2coco = dict([(v,k) for k,v in coco2vg.items()])\n",
    "\n",
    "visgencocap_objdf = df['vgobjdf'].merge(pd.DataFrame(vgcap_vg_iids, columns=['image_id']))\n",
    "\n",
    "vgobjdf_syned = df['vgobjdf'][~df['vgobjdf']['syn'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up the nearest neighbour index of captions in embedding space\n",
    "ind = AnnoyIndex(512, metric='euclidean')\n",
    "ind.load(preproc_path + '/caps.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding index works via the row number in cococapdf, so need mapping\n",
    "\n",
    "# N.B.: this is actually lossy. This is the row of the *first* caption for that\n",
    "#  image. But there will also be other rows (captions) for same image.\n",
    "#  So, should really one to many mapping. But for purposes here, is ok.\n",
    "coco2row = dict(zip(df['cococapdf']['image_id'].tolist(), df['cococapdf'].index.tolist()))\n",
    "# This is also the reason which this here is not inverse of above:\n",
    "row2coco = dict(zip(df['cococapdf'].index.tolist(), df['cococapdf']['image_id'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption / Object\n",
    "\n",
    "Given the situation described by the caption, could this object be present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "capobj_df = pd.read_csv('EntailOut/capobj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>hypothesis_syn</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>348816</td>\n",
       "      <td>A small white dog sitting on a piece of luggage.</td>\n",
       "      <td>scratch</td>\n",
       "      <td>abrasion.n.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>261196</td>\n",
       "      <td>A table has a meal placed on top of it.</td>\n",
       "      <td>cardboard</td>\n",
       "      <td>cardboard.n.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                           premise hypothesis  \\\n",
       "0    348816  A small white dog sitting on a piece of luggage.    scratch   \n",
       "1    261196           A table has a meal placed on top of it.  cardboard   \n",
       "\n",
       "   hypothesis_syn  label  \n",
       "0   abrasion.n.01      1  \n",
       "1  cardboard.n.01      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capobj_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict via retrieved models\n",
    "\n",
    "The idea is that given the premise, related images (= models) are retrieved, based on the similarity of their caption to that premise. That is, we first do an image retrieval task via captions. Then we check whether the object mentioned in the hypothesis is present in the retrieved images, and via that, answer the question whether it is also likely to be present in the situation described by the premise.\n",
    "\n",
    "A hyperparameter is the threshold above which we say yes. If set to 0.2, this means that we are happy if it is in 20% of the retrieved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_captions(background_data, this_cocoii, nns_max_try=50, nns_max=10):\n",
    "    '''Retrieve some nearest captions, ensuring that they are from different image\n",
    "    '''\n",
    "    ind, coco2row, coco2vg, vgcap_coco_iids = background_data\n",
    "    # this works on row numbers in cococap\n",
    "    this_row = coco2row[this_cocoii]\n",
    "    nns = ind.get_nns_by_item(this_row, nns_max_try)\n",
    "    # translate back to coco image_ids\n",
    "    nns = [row2coco[nn] for nn in nns]\n",
    "    # it's quite likely that the other captions of the same image\n",
    "    # ended up very similar. Mapped to coco image_ids, that means\n",
    "    # that we may have same id multiple times in this list. \n",
    "    # To filter these out & ensure that we really retrieve different\n",
    "    # images, and do this w/o changing order (as list(set(.)) would do)\n",
    "    seen = set()\n",
    "    seen.add(this_cocoii) # make sure that seed image is filtered out\n",
    "    nns_filtered = []\n",
    "    for nn in nns:\n",
    "        if nn not in seen:\n",
    "            nns_filtered.append(nn)\n",
    "            seen.add(nn)\n",
    "    # filter so that only coco ids in visual genome remain\n",
    "    nns_filtered = [nn for nn in nns_filtered if nn in vgcap_coco_iids]\n",
    "    # at most nns_max \n",
    "    nns_filtered = nns_filtered[:min(nns_max, len(nns_filtered))]\n",
    "    # finally, translate into vg image_ids\n",
    "    nns_filtered = [int(coco2vg[e]) for e in nns_filtered]\n",
    "    # returns vg image_ids \n",
    "    return nns_filtered\n",
    "\n",
    "def extract_model(objdf, pregdf, ii):\n",
    "    ic = icorpus_code['visual_genome']\n",
    "    individuals = query_by_id(objdf, (ic, ii), 'obj_id syn name'.split()).values\n",
    "    D = [e[0] for e in individuals]\n",
    "    I_indv = defaultdict(list)\n",
    "    _ = [I_indv[osyn].append(oid) for oid, osyn, _ in individuals if osyn is not None]\n",
    "    \n",
    "    relations = [r for rs in query_by_id(pregdf, (ic, ii), 'rels') for r in rs]\n",
    "    I_rels = defaultdict(list)\n",
    "    _ = [I_rels[rsyn].append((sid, oid)) for sid, _, rsyn, oid in relations if rsyn is not None]\n",
    "    return D, I_indv, I_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_data = (ind, coco2row, coco2vg, vgcap_coco_iids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example. Retrieve a related model (type). Given a target caption, retrieve similar captions, and from the image linked to one of these, retrieve all object and relation types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of D: 61\n",
      "object types: [u'backpack.n.01', u'mouth.n.01', u'head.n.01', u'woman.n.01', u'leash.n.01', u'skirt.n.01', u'dog.n.01', u'eye.n.01', u'ear.n.01', u'nose.n.01', u'handle.n.01', u'slide_fastener.n.01', u'bag.n.01', u'tongue.n.01', u'pocket.n.01', u'vest.n.01', u'battalion.n.02', u'television.n.01', u'man.n.01', u'jacket.n.01', u'sign.n.02', u'hair.n.01']\n",
      "relation types: [u'have.v.01', u'along.r.01', u'wear.v.01', u'transport.v.02']\n"
     ]
    }
   ],
   "source": [
    "this_ii = capobj_df.iloc[0,0]\n",
    "nns_filtered = retrieve_captions(background_data, this_ii, nns_max_try=50, nns_max=10)\n",
    "\n",
    "D, I_indv, I_rels = extract_model(df['vgobjdf'], df['vgpregdf'], nns_filtered[3])\n",
    "print(\"size of D:\", len(D))\n",
    "print(\"object types:\", I_indv.keys())\n",
    "print(\"relation types:\", I_rels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_print(message, debug):\n",
    "    if debug:\n",
    "        print(message)\n",
    "    \n",
    "def predict_via_retrieval(background_data, this_row, debug=False,\n",
    "                          threshold=0.2, nns_max=6):\n",
    "    ind, coco2row, coco2vg, vgcap_coco_iids = background_data\n",
    "    \n",
    "    coco_ii, cap, hyp, hyp_syn, label = this_row\n",
    "    \n",
    "    debug_print(\"=\" * 60, debug)\n",
    "    debug_print(this_row, debug)\n",
    "    \n",
    "    nns_filtered = retrieve_captions(background_data, coco_ii, nns_max=nns_max)\n",
    "    \n",
    "    score = 0\n",
    "    debug_print(\"retrieved:\", debug)\n",
    "    for this_nn in nns_filtered:\n",
    "        debug_print('  ' + str(this_nn) + ' ' + df['cococapdf'].iloc[coco2row[vg2coco[this_nn]]]['caption'], debug)\n",
    "        D, I_indv, I_rels = extract_model(df['vgobjdf'], df['vgpregdf'], this_nn)\n",
    "        if hyp_syn in I_indv.keys():\n",
    "            debug_print( '       match', debug)\n",
    "            score += 1\n",
    "    score /= nns_max\n",
    "    debug_print( \"score: {}\".format(score), debug)\n",
    "    \n",
    "    return 1 if ((score > threshold and label == 1) or (score <= threshold and label == 0)) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "image_id                                                                             519374\n",
      "premise           He is expertly using his skateboard to go in and out of the street cones.\n",
      "hypothesis                                                                              man\n",
      "hypothesis_syn                                                                     man.n.01\n",
      "label                                                                                     1\n",
      "Name: 19, dtype: object\n",
      "retrieved:\n",
      "  2317174 Two skateboarders make there way through a cone obstacle course.\n",
      "       match\n",
      "  2392316 Skate boarder keeping balance while navigating green cones\n",
      "       match\n",
      "  286083 A man in blue jeans and a helmet rides a skateboard near two green cones.\n",
      "  2318884 A young man is skateboarding down a hill.\n",
      "       match\n",
      "  2368945 The young man is going around the cone on his skateboard.\n",
      "       match\n",
      "  2384481 A skateboarder doing a trick on a ramp in the evening.\n",
      "       match\n",
      "score: 0.833333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_via_retrieval(background_data, capobj_df.iloc[19], debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is very slow, so am not rerunning this here... See the (less commented) original notebook `model_building_orig.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6h 35min 14s, sys: 44min 46s, total: 7h 20min\n",
      "Wall time: 1h 42min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "capobj_scores = capobj_df.apply(lambda x: predict_via_retrieval(background_data, x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(scores):\n",
    "    return scores.sum() / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'capobj_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d9cd4d6033c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapobj_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'capobj_scores' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(capobj_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict via string matching\n",
    "\n",
    "Baseline 1: Is the object name mentioned in the premise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_via_string(this_row, mode='prediction'):\n",
    "    _coco_ii, cap, hyp, _hyp_syn, label = this_row\n",
    "    return ( 1 if ((hyp in cap) and (label == 1)) or \n",
    "                   ((hyp not in cap) and (label == 0)) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_bsln = capobj_df.apply(lambda x: predict_via_string(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57505"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(scores_bsln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict via embedding distance\n",
    "\n",
    "Baseline 2: Predict via euclidean distance in the embedding space, between the caption (premise) and the object name (hypothesis). We choose the threshold distance for making the decision so that it partitions the set (as we know that the test set is balanced). That is, we choose it in such a way that it assigns \"yes\" to 10k instances and \"no\" to the 10k others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "objvecs = np.load('EntailOut/capobj.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "capvecs = np.load(preproc_path + '/cap_embeds.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3174902200698853"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_n = 17\n",
    "coco_ii, cap, hyp, hyp_syn, label = capobj_df.iloc[row_n]\n",
    "objvec = objvecs[row_n]\n",
    "capvec = capvecs[coco2row[coco_ii]]\n",
    "euclidean(objvec, capvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "capvec_indices = [coco2row[e] for e in capobj_df['image_id'].tolist()]\n",
    "\n",
    "capvec_matrix = capvecs[capvec_indices]\n",
    "# subset of caption embedding matrix, in order of test data set\n",
    "\n",
    "distances = np.sqrt(np.sum((capvec_matrix - objvecs)**2, axis=1))\n",
    "# vector of distances between caption and object name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10191"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set threshold so that it partitions set\n",
    "threshold = 1.27\n",
    "(distances < threshold).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64105"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((capobj_df[distances < threshold]['label'] == 1).sum() + \n",
    " (capobj_df[distances >= threshold]['label'] == 0).sum()) / len(capobj_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption / Region\n",
    "\n",
    "Now we do the same with pairs of caption (hypothesis) and region (premise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "capreg_df = pd.read_csv('EntailOut/capreg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the relations as read from csv are strings; must be turned into objects\n",
    "capreg_df['hyp_rel'] = capreg_df['hyp_rel'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>hyp_rel</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>421813</td>\n",
       "      <td>A young boy holding a donut with pink sprinkles on it.</td>\n",
       "      <td>boats in the water</td>\n",
       "      <td>[[544272, IN, in.r.01, 544263]]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290935</td>\n",
       "      <td>The young man is riding his skateboard down the street.</td>\n",
       "      <td>A man in a green shirt.</td>\n",
       "      <td>[[2555677, IN, in.r.01, 3904012]]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                                   premise  \\\n",
       "0    421813    A young boy holding a donut with pink sprinkles on it.   \n",
       "1    290935  The young man is riding his skateboard down the street.    \n",
       "\n",
       "                 hypothesis                            hyp_rel  label  \n",
       "0        boats in the water    [[544272, IN, in.r.01, 544263]]      0  \n",
       "1  A man in a green shirt.   [[2555677, IN, in.r.01, 3904012]]      0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capreg_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict via retrieved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_syn(df, obj_id, key='syn'):\n",
    "    return df[df['obj_id'] == obj_id][key].values[0]\n",
    "\n",
    "def rels2model_type(objdf, rels):\n",
    "    ind_types = []\n",
    "    rel_types = []\n",
    "    for sid, _, rsyn, oid in rels:\n",
    "        [ind_types.append(get_obj_syn(objdf, this_id)) for this_id in [sid, oid]]\n",
    "        rel_types.append(rsyn)\n",
    "    return set(ind_types), set(rel_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({u'curtain.n.01', u'piano.n.01'}, {u'behind.r.01'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_ii, cap, hyp, hyp_rel, label = capreg_df.iloc[10]\n",
    "rels2model_type(df['vgobjdf'], hyp_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(I_indiv, I_rels, hyp_indiv, hyp_rels):\n",
    "    score_ind = len(set(I_indiv.keys()).intersection(hyp_indiv)) / len(hyp_indiv)\n",
    "    score_rel = len(set(I_rels.keys()).intersection(hyp_rels)) / len(hyp_rels)\n",
    "    return (score_ind + score_rel) / 2\n",
    "\n",
    "def predict_via_retrieval_region(background_data, this_row, debug=False,\n",
    "                                 threshold=0.2, nns_max=6):\n",
    "    ind, coco2row, coco2vg, vgcap_coco_iids = background_data\n",
    "    \n",
    "    coco_ii, cap, hyp, hyp_rel, label = this_row\n",
    "    debug_print(\"=\" * 60, debug)\n",
    "    debug_print((cap, hyp, label), debug)\n",
    "    \n",
    "    nns_filtered = retrieve_captions(background_data, coco_ii, nns_max=nns_max)\n",
    "    \n",
    "    hyp_indiv, hyp_rels = rels2model_type(df['vgobjdf'], hyp_rel)\n",
    "    debug_print('hyp_ind: {}  hyp_rels: {}'.format(hyp_indiv, hyp_rels), debug)\n",
    "\n",
    "    score = 0\n",
    "    debug_print(\"retrieved:\", debug)\n",
    "    for this_nn in nns_filtered:\n",
    "        debug_print('  ' + str(this_nn) + ' ' + df['cococapdf'].iloc[coco2row[vg2coco[this_nn]]]['caption'], debug)\n",
    "        D, I_indv, I_rels = extract_model(df['vgobjdf'], df['vgpregdf'], this_nn)\n",
    "        \n",
    "        score += score_model(I_indv, I_rels, hyp_indiv, hyp_rels)\n",
    "\n",
    "    score /= len(nns_filtered)\n",
    "    debug_print( \"score: {}\".format(score), debug)\n",
    "    \n",
    "    return 1 if ((score > threshold and label == 1) or (score <= threshold and label == 0)) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "('A piece of cake with icing on a plate. ', 'a girl wearing a white shirt', 0)\n",
      "hyp_ind: set([u'girl.n.01', u'shirt.n.01'])  hyp_rels: set([u'wear.v.01'])\n",
      "retrieved:\n",
      "  2365845 Apples and dessert are plated on a table.\n",
      "  2392234 A white plate with a piece of cake next to a puff of whipped cream.\n",
      "  2391081 A small piece of cake sits beside a fork on a tiny plate.\n",
      "  2384101 A piece of pastry is speared by a fork.\n",
      "  2329260 A plate with a slice of dessert with whipped cream.\n",
      "  2323507 A table with a white plate and a cake.\n",
      "score: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_via_retrieval_region(background_data, capreg_df.iloc[4], debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6h 39min 32s, sys: 44min 57s, total: 7h 24min 30s\n",
      "Wall time: 1h 43min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "capreg_scores = capreg_df.apply(lambda x: predict_via_retrieval_region(background_data, x),\n",
    "                               axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64875"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(capreg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict via string matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(phrase_A, phrase_B):\n",
    "    set_A = set(phrase_A.split())\n",
    "    set_B = set(phrase_B.split())\n",
    "    return len(set_A.intersection(set_B)) / len(set_A.union(set_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_via_jaccard(this_row, threshold=0.2):\n",
    "    coco_ii, cap, hyp, hyp_rel, label = this_row\n",
    "    return 1 if (((jaccard_sim(cap, hyp) >= threshold) and label == 1) or\n",
    "                 ((jaccard_sim(cap, hyp) < threshold and label == 0))) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_bsln_reg = capreg_df.apply(lambda x: predict_via_jaccard(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54435"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(scores_bsln_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict via embedding distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "regvecs = np.load('EntailOut/capreg.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_reg = np.sqrt(np.sum((capvec_matrix - regvecs)**2, axis=1))\n",
    "# vector of distances between caption and region embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10307"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set threshold so that it partitions set\n",
    "threshold_reg = 1.245\n",
    "(distances_reg < threshold_reg).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50255"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((capreg_df[distances_reg < threshold_reg]['label'] == 1).sum() + \n",
    " (capreg_df[distances_reg >= threshold_reg]['label'] == 0).sum()) / len(capreg_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
