{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = './Results_Anon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'HITId', u'HITTypeId', u'Title', u'Description',\n",
       "       u'Keywords', u'Reward', u'CreationTime', u'MaxAssignments',\n",
       "       u'RequesterAnnotation', u'AssignmentDurationInSeconds',\n",
       "       u'AutoApprovalDelayInSeconds', u'Expiration', u'NumberOfSimilarHITs',\n",
       "       u'LifetimeInSeconds', u'AssignmentId', u'WorkerId', u'AssignmentStatus',\n",
       "       u'AcceptTime', u'SubmitTime', u'AutoApprovalTime', u'ApprovalTime',\n",
       "       u'RejectionTime', u'RequesterFeedback', u'WorkTimeInSeconds',\n",
       "       u'LifetimeApprovalRate', u'Last30DaysApprovalRate',\n",
       "       u'Last7DaysApprovalRate', u'Input.premise', u'Input.hypothesis',\n",
       "       u'Input.label', u'Input.prompt', u'Answer.semantic-similarity.label',\n",
       "       u'Approve', u'Reject'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pd.read_csv(results_path + '/rexrex_Batch_3501801_batch_results_anon.csv')\n",
    "example.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Assume Text 1 is meant to refer to something (which could be a person, an animal, or a thing) in a picture. Could Text 2 refer to the same thing? (That is, could Text 2 be a clarification of Text 1, in some situation?)',\n",
       " array(['1 - Not At All', '3', '4 - Yes, absolutely', '2'], dtype=object))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['Input.prompt'][0], example['Answer.semantic-similarity.label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_tuple_list(df, group_by='HITId', summariser='avg'):\n",
    "    df['anno_label'] = df['Answer.semantic-similarity.label'].apply(lambda x: int(x[0]))\n",
    "    grouped = df.groupby(group_by)\n",
    "    tuples = []\n",
    "    for group in grouped.groups.keys():\n",
    "        this_slice = grouped.get_group(group)\n",
    "        if summariser == 'avg':\n",
    "            avg_label = this_slice['anno_label'].mean()\n",
    "        elif summariser == 'majority':\n",
    "            avg_label = np.bincount(this_slice['anno_label'].values).argmax()\n",
    "        elif summariser == 'rand':\n",
    "            avg_label = this_slice['anno_label'].sample().values[0]\n",
    "        std_label = this_slice['anno_label'].std()\n",
    "        n_rows = len(this_slice)\n",
    "        hyp = this_slice['Input.hypothesis'].tolist()[0]\n",
    "        prem = this_slice['Input.premise'].tolist()[0]\n",
    "        true_label = this_slice['Input.label'].tolist()[0]\n",
    "        tuples.append((group, prem, hyp, true_label, avg_label, std_label, n_rows))\n",
    "    return tuples\n",
    "\n",
    "def score_res(df, summariser='avg', break_point=3):\n",
    "    tuples = prep_tuple_list(df, summariser=summariser)\n",
    "    acc = sum([1 if (avg >= break_point and lab == 1) or (avg < break_point and lab == 0) else 0\n",
    "               for _, _, _, lab, avg, _, _ in tuples]) / len(tuples)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "./Results_Anon/capcap_Batch_3501811_batch_results_anon.csv\n",
      "\n",
      "Is Text 2 likely to be describing the same situation as Text 1?\n",
      "['3' '2' '4 - Yes, absolutely' '1 - Not At All']\n",
      "\n",
      "    avg: 0.63\n",
      "    maj: 0.6\n",
      "    rnd: 0.63\n",
      "Avg 0: 2.5  Avg 1: 2.9\n",
      "    raw: 0.59\n",
      "# diff annotators 41\n",
      "------------------------------------------------------------\n",
      "./Results_Anon/capdeep_Batch_3501826_batch_results_anon.csv\n",
      "\n",
      "Is Text 2 likely to be a longer description of the situation described by Text 1?\n",
      "['4 - Yes, absolutely' '3' '1 - Not At All' '2']\n",
      "\n",
      "    avg: 0.6\n",
      "    maj: 0.48\n",
      "    rnd: 0.52\n",
      "Avg 0: 2.7  Avg 1: 2.9\n",
      "    raw: 0.54\n",
      "# diff annotators 33\n",
      "------------------------------------------------------------\n",
      "./Results_Anon/capobj_Batch_3501814_batch_results_anon.csv\n",
      "\n",
      "Using what you know about the world, in the situation described by Text 1, is Text 2 likely to be true?\n",
      "['4 - Yes, absolutely' '1 - Not At All' '2' '3']\n",
      "\n",
      "    avg: 0.58\n",
      "    maj: 0.55\n",
      "    rnd: 0.55\n",
      "Avg 0: 2.2  Avg 1: 2.6\n",
      "    raw: 0.56\n",
      "# diff annotators 41\n",
      "------------------------------------------------------------\n",
      "./Results_Anon/capreg_Batch_3501818_batch_results_anon.csv\n",
      "\n",
      "Using what you know about the world, in the situation described by Text 1, is Text 2 likely to be true?\n",
      "['2' '3' '4 - Yes, absolutely' '1 - Not At All']\n",
      "\n",
      "    avg: 0.6\n",
      "    maj: 0.58\n",
      "    rnd: 0.62\n",
      "Avg 0: 2.2  Avg 1: 2.5\n",
      "    raw: 0.58\n",
      "# diff annotators 43\n",
      "------------------------------------------------------------\n",
      "./Results_Anon/rexrex_Batch_3501801_batch_results_anon.csv\n",
      "\n",
      "Assume Text 1 is meant to refer to something (which could be a person, an animal, or a thing) in a picture. Could Text 2 refer to the same thing? (That is, could Text 2 be a clarification of Text 1, in some situation?)\n",
      "['1 - Not At All' '3' '4 - Yes, absolutely' '2']\n",
      "\n",
      "    avg: 0.68\n",
      "    maj: 0.72\n",
      "    rnd: 0.72\n",
      "Avg 0: 1.8  Avg 1: 2.7\n",
      "    raw: 0.69\n",
      "# diff annotators 51\n"
     ]
    }
   ],
   "source": [
    "for respath in glob(results_path + '/*csv'):\n",
    "    break_point = 3\n",
    "    print('-' * 60)\n",
    "    print(respath)\n",
    "    df = pd.read_csv(respath)\n",
    "    print()\n",
    "    print(df['Input.prompt'][0])\n",
    "    print(df['Answer.semantic-similarity.label'].unique())\n",
    "    print('')\n",
    "    print('    avg: {:.2}'.format(score_res(df, break_point=break_point)))\n",
    "    print('    maj: {:.2}'.format(score_res(df, summariser='majority', break_point=break_point)))\n",
    "    print('    rnd: {:.2}'.format(score_res(df, summariser='rand', break_point=break_point)))\n",
    "    \n",
    "    print('Avg 0: {:.2}  Avg 1: {:.2}'.format(df[df['Input.label'] == 0]['anno_label'].mean(), \n",
    "                                                        df[df['Input.label'] == 1]['anno_label'].mean()))\n",
    "    correctdf = df[((df['anno_label'] >= break_point) & (df['Input.label'] == 1)) |\n",
    "                   ((df['anno_label'] < break_point) & (df['Input.label'] == 0))]\n",
    "    print('    raw: {:.2}'.format(len(correctdf) / len(df)))\n",
    "    print('# diff annotators', len(pd.read_csv(respath)['WorkerId'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
