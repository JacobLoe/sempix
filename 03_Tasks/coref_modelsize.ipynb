{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*David Schlangen, 2019-03-24*\n",
    "\n",
    "# Task: Resolving Co-Reference / Predicting Model Size\n",
    "\n",
    "In the section on discourses in the denotations section, we have already briefly mentioned the task of co-reference resolution. If the image (model) is available, co-reference is established exophorically, via the anchoring in the image object. If we take away the image, the task must be tackled via linguistic evidence (and common-sense knowledge about scenes) alone. (It hence becomes an inference / entailment task more than one of denotation computation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from __future__ import division\n",
    "import codecs\n",
    "import json\n",
    "from itertools import chain, izip, permutations, combinations\n",
    "from collections import Counter, defaultdict\n",
    "import ConfigParser\n",
    "import os\n",
    "import random\n",
    "from textwrap import fill\n",
    "import scipy\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "from nltk.parse import CoreNLPParser\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Latex, display\n",
    "\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# Load up config file (needs path; adapt env var if necessary); local imports\n",
    "\n",
    "# load config file, set up paths, make project-specific imports\n",
    "config_path = os.environ.get('VISCONF')\n",
    "if not config_path:\n",
    "    # try default location, if not in environment\n",
    "    default_path_to_config = '../Config/default.cfg'\n",
    "    if os.path.isfile(default_path_to_config):\n",
    "        config_path = default_path_to_config\n",
    "\n",
    "assert config_path is not None, 'You need to specify the path to the config file via environment variable VISCONF.'        \n",
    "\n",
    "config = ConfigParser.SafeConfigParser()\n",
    "with codecs.open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config.readfp(f)\n",
    "\n",
    "corpora_base = config.get('DEFAULT', 'corpora_base')\n",
    "preproc_path = config.get('DSGV-PATHS', 'preproc_path')\n",
    "dsgv_home = config.get('DSGV-PATHS', 'dsgv_home')\n",
    "\n",
    "\n",
    "sys.path.append(dsgv_home + '/Utils')\n",
    "from utils import icorpus_code, plot_labelled_bb, get_image_filename, query_by_id\n",
    "from utils import plot_img_cropped, plot_img_ax, invert_dict, get_a_by_b\n",
    "sys.path.append(dsgv_home + '/WACs/WAC_Utils')\n",
    "from wac_utils import create_word2den, is_relational\n",
    "sys.path.append(dsgv_home + '/Preproc')\n",
    "from sim_preproc import load_imsim, n_most_sim\n",
    "\n",
    "sys.path.append('../Common')\n",
    "from data_utils import load_dfs, plot_rel_by_relid, get_obj_bb, compute_distance_objs\n",
    "from data_utils import get_obj_key, compute_relpos_relargs_row, get_all_predicate\n",
    "from data_utils import compute_distance_relargs_row, get_rel_type, get_rel_instances\n",
    "from data_utils import compute_obj_sizes_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# Load up preprocessed DataFrames. Slow!\n",
    "# These DataFrames are the result of pre-processing the original corpus data,\n",
    "# as per dsg-vision/Preprocessing/preproc.py\n",
    "\n",
    "df_names = ['vgregdf', #'vgimgdf', 'vgobjdf', 'vgreldf',\n",
    "           ]\n",
    "df = load_dfs(preproc_path, df_names)\n",
    "\n",
    "# a derived DF, containing only those region descriptions which I was able to resolve\n",
    "df['vgpregdf'] = df['vgregdf'][df['vgregdf']['pphrase'].notnull() & \n",
    "                               (df['vgregdf']['pphrase'] != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-reference resolution is the task of determining whether a referring expression introduces a new entity into the discourse or not. We can create data for this task using the visual genome region annotation. Turning the set of region descriptions into a \"discourse\", we have gold truth information about whether a region description that is  added to the discourse introduces a new entity or talks about one that has previously been introduced.\n",
    "\n",
    "This is what a model would have to predict. The result then is a set of co-reference chains, or entity mentions (in order of ocurrence). From a more semantic point of view, the task entails determining the size of the intended model of the discourse; co-reference between two mentions here means that only one individual constant needs to be introduced into the model. This is how it is displayed below, with the maximal model size being the number of entity-denoting expressions (if we were to create a new individual constant for each), the minimal number being the number of entity-types in the discourse (and assuming that all mentions of the same type co-refer), and the actual size being the one indicated by the object resolution of the descriptions. A perfect resolution of the co-references would lead to that number. \n",
    "\n",
    "(Note that the example here is relies on the provided object identifiers to distinguish objects, but visual genome seems to have insuffiently consolidated on that score. To create a cleaner dataset, to make the judgement whether a new object is introduced or not, a test of overlap (intersection over union) between bounding boxes should be performed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "black tie worn by man\n",
      "    NEW: 1167150 (necktie.n.01)\n",
      "    NEW: 1167149 (man.n.01)\n",
      "----------\n",
      "logo on white shirt worn by man\n",
      "    NEW: 1167151 (logo.n.01)\n",
      "    NEW: 1167152 (shirt.n.01)\n",
      "----------\n",
      "black glasses worn by man\n",
      "    NEW: 1167153 (spectacles.n.01)\n",
      "    OLD: 1167149 (man.n.01)\n",
      "----------\n",
      "collar of white shirt worn by man\n",
      "    NEW: 1167155 (collar.n.01)\n",
      "    OLD: 1167152 (shirt.n.01)\n",
      "----------\n",
      "collar of white shirt worn by man\n",
      "    OLD: 1167155 (collar.n.01)\n",
      "    OLD: 1167152 (shirt.n.01)\n",
      "----------\n",
      "white shirt worn by man\n",
      "    OLD: 1167152 (shirt.n.01)\n",
      "    OLD: 1167149 (man.n.01)\n",
      "----------\n",
      "white shirt worn by man\n",
      "    OLD: 1167152 (shirt.n.01)\n",
      "    OLD: 1167149 (man.n.01)\n",
      "----------\n",
      "an epaulet on a shirt\n",
      "    NEW: 1167160 (epaulet.n.01)\n",
      "    NEW: 1167158 (shirt.n.01)\n",
      "    old type, new instance: 1167158 shirt.n.01\n",
      "----------\n",
      "a collar on a shirt\n",
      "    OLD: 1167155 (collar.n.01)\n",
      "    OLD: 1167158 (shirt.n.01)\n",
      "----------\n",
      "glasses on a man's face\n",
      "    NEW: 1167156 (spectacles.n.01)\n",
      "    old type, new instance: 1167156 spectacles.n.01\n",
      "    NEW: 1167161 (face.n.01)\n",
      "----------\n",
      "a white shirt on a man\n",
      "    OLD: 1167152 (shirt.n.01)\n",
      "    OLD: 1167149 (man.n.01)\n",
      "----------\n",
      "an eyebrow on a man's face\n",
      "    NEW: 1167167 (eyebrow.n.01)\n",
      "    OLD: 1167161 (face.n.01)\n",
      "----------\n",
      "black tie on a shirt\n",
      "    OLD: 1167150 (necktie.n.01)\n",
      "    OLD: 1167158 (shirt.n.01)\n",
      "----------\n",
      "mouth on a man\n",
      "    NEW: 1167168 (mouth.n.01)\n",
      "    OLD: 1167149 (man.n.01)\n",
      "----------\n",
      "white lapel on a shirt\n",
      "    NEW: 1167169 (lapel.n.01)\n",
      "    OLD: 1167158 (shirt.n.01)\n",
      "----------\n",
      "glasses on a man\n",
      "    OLD: 1167156 (spectacles.n.01)\n",
      "    OLD: 1167149 (man.n.01)\n",
      "----------\n",
      "left ear of a man\n",
      "    NEW: 1167171 (ear.n.01)\n",
      "    OLD: 1167149 (man.n.01)\n",
      "----------\n",
      "striped curtain in the background\n",
      "    NEW: 1167172 (curtain.n.01)\n",
      "    NEW: 1167173 (background.n.02)\n",
      "----------\n",
      "yellow wall behind man\n",
      "    OLD: 1167173 (wall.n.01)\n",
      "    OLD: 1167149 (man.n.01)\n",
      "----------\n",
      "a pair of glasses on a man.\n",
      "    OLD: 1167156 (spectacles.n.01)\n",
      "    OLD: 1167149 (man.n.01.)\n",
      "----------\n",
      "the left ear of a man.\n",
      "    OLD: 1167171 (ear.n.01)\n",
      "    OLD: 1167149 (man.n.01.)\n",
      "----------\n",
      "max model size: 42 || min model size: 14 || actual model size: 16\n",
      "\n",
      "======================================================================\n",
      "Zebra running in the pasture. \n",
      "    NEW: 3181384 (zebra.n.01)\n",
      "    NEW: 3630046 (pasture.n.01.)\n",
      "----------\n",
      "The zebra's ears. \n",
      "    NEW: 2155639 (zebra.n.01's)\n",
      "    NEW: 3630047 (ear.n.01)\n",
      "    NEW: 3630048 (ear.n.01.)\n",
      "----------\n",
      "Snow on the ground. \n",
      "    NEW: 2137059 (snow.n.01)\n",
      "    NEW: 2137060 (land.n.04.)\n",
      "----------\n",
      "Rocks and pebbles line the fence. \n",
      "    NEW: 2520579 (rock.n.01)\n",
      "    NEW: 2555101 (fence.n.01.)\n",
      "----------\n",
      "Another animal in the background. \n",
      "    NEW: 3513793 (animal.n.01)\n",
      "    NEW: 3644159 (background.n.02.)\n",
      "----------\n",
      "Grass for the zebra to munch on. \n",
      "    NEW: 2473866 (grass.n.01)\n",
      "    OLD: 2155639 (zebra.n.01)\n",
      "----------\n",
      "Eye of the zebra\n",
      "    NEW: 2419019 (eye.n.01)\n",
      "    NEW: 3191951 (zebra.n.01)\n",
      "    old type, new instance: 3191951 zebra.n.01\n",
      "----------\n",
      "Black and white mane of the zebra\n",
      "    NEW: 2166337 (mane.n.01)\n",
      "    OLD: 2155639 (zebra.n.01)\n",
      "----------\n",
      "zebra's striped mane on neck\n",
      "    NEW: 3411714 (mane.n.01)\n",
      "    old type, new instance: 3411714 mane.n.01\n",
      "    NEW: 3411715 (neck.n.01)\n",
      "----------\n",
      "zebra trotting across green field\n",
      "    OLD: 2155639 (zebra.n.01)\n",
      "    NEW: 2798444 (field.n.01)\n",
      "----------\n",
      "zebra walking across field\n",
      "    OLD: 2155639 (zebra.n.01)\n",
      "    OLD: 2798444 (field.n.01)\n",
      "----------\n",
      "A zebra walks along the grass\n",
      "    NEW: 3209008 (zebra.n.01)\n",
      "    old type, new instance: 3209008 zebra.n.01\n",
      "    NEW: 3546631 (grass.n.01)\n",
      "    old type, new instance: 3546631 grass.n.01\n",
      "----------\n",
      "Rocks piled on the ground\n",
      "    NEW: 3047808 (rock.n.01)\n",
      "    old type, new instance: 3047808 rock.n.01\n",
      "    NEW: 3630055 (land.n.04)\n",
      "----------\n",
      "A tree in the park\n",
      "    NEW: 2220751 (tree.n.01)\n",
      "    NEW: 2220752 (park.n.01)\n",
      "----------\n",
      "Two leafless trees in the park\n",
      "    NEW: 3661066 (tree.n.01)\n",
      "    old type, new instance: 3661066 tree.n.01\n",
      "    NEW: 3661067 (tree.n.01)\n",
      "    old type, new instance: 3661067 tree.n.01\n",
      "    NEW: 3661068 (park.n.01)\n",
      "    old type, new instance: 3661068 park.n.01\n",
      "----------\n",
      "A twig stands upright in the grass\n",
      "    NEW: 2773996 (branchlet.n.01)\n",
      "    NEW: 3661071 (grass.n.01)\n",
      "    old type, new instance: 3661071 grass.n.01\n",
      "----------\n",
      "max model size: 34 || min model size: 20 || actual model size: 29\n",
      "\n",
      "======================================================================\n",
      "The lid of the suitcase\n",
      "    NEW: 1255863 (eyelid.n.01)\n",
      "    NEW: 1255857 (bag.n.06)\n",
      "----------\n",
      "The black wheel of the suitcase\n",
      "    NEW: 1255864 (wheel.n.01)\n",
      "    OLD: 1255857 (bag.n.06)\n",
      "----------\n",
      "Baby sitting in a suitcase.  \n",
      "    NEW: 1255855 (baby.n.01)\n",
      "    OLD: 1255857 (bag.n.06.)\n",
      "----------\n",
      "The baby has red hair.  \n",
      "    OLD: 1255855 (baby.n.01)\n",
      "    NEW: 1255856 (hair.n.01.)\n",
      "----------\n",
      "Baby is wearing a bib.\n",
      "    OLD: 1255855 (baby.n.01)\n",
      "    NEW: 1255869 (bib.n.01.)\n",
      "----------\n",
      "ring on hand of person\n",
      "    NEW: 1255867 (ring.n.01)\n",
      "    NEW: 1255866 (hand.n.01)\n",
      "    NEW: 1255859 (person.n.01)\n",
      "----------\n",
      "max model size: 13 || min model size: 9 || actual model size: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# deep caption with co-reference on object level\n",
    "def extr_disc_ref_pphr(pphr):\n",
    "    discourse_referents = []\n",
    "    for token in pphr.split():\n",
    "        subtoken = token.split('|')\n",
    "        if len(subtoken) > 1:\n",
    "            word = subtoken.pop(0)\n",
    "            id_syn_list = zip(subtoken[::2], subtoken[1::2])\n",
    "            discourse_referents.extend([(int(e[0]), e[1]) for e in id_syn_list])\n",
    "    return discourse_referents\n",
    "\n",
    "def cond_print(instr, show):\n",
    "    if show:\n",
    "        print instr\n",
    "\n",
    "def model_size_stats(df, image_id, show=False):\n",
    "    all_pphr = df[df['image_id'] == image_id][['phrase', 'pphrase']].values.tolist()\n",
    "    all_discourse_referents = []\n",
    "    all_types = set()\n",
    "    n_mentions = 0\n",
    "    for this_phr, this_pphr in all_pphr:\n",
    "        cond_print(this_phr, show)\n",
    "        this_disc_refs = extr_disc_ref_pphr(this_pphr)\n",
    "        n_mentions += len(this_disc_refs)\n",
    "        #print '   ', this_disc_refs\n",
    "        #this_disc_refs_ids, this_disc_ref_types = zip(*this_disc_refs)\n",
    "        for disc_ref, ref_type in this_disc_refs:\n",
    "            if disc_ref in all_discourse_referents:\n",
    "                cond_print(\"    OLD: %d (%s)\" % (disc_ref, ref_type), show)\n",
    "            else:\n",
    "                cond_print(\"    NEW: %d (%s)\" % (disc_ref, ref_type), show)\n",
    "                all_discourse_referents.append(disc_ref)\n",
    "                if ref_type in all_types:\n",
    "                    cond_print('    old type, new instance: %d %s' % (disc_ref, ref_type), show)\n",
    "                all_types.add(ref_type)\n",
    "        cond_print('-' * 10, show)\n",
    "    cond_print('max model size: %d || min model size: %d || actual model size: %d'\\\n",
    "                    % (n_mentions, len(all_types), len(all_discourse_referents)), show)\n",
    "    return n_mentions, len(all_types), len(all_discourse_referents)\n",
    "\n",
    "n_egs = 3\n",
    "\n",
    "for _ in range(n_egs):\n",
    "    print \"=\" * 70\n",
    "    ii = df['vgpregdf'].sample()['image_id'].values[0]\n",
    "    model_size_stats(df['vgpregdf'], ii, show=True)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the examples here show, these aren't particularly nice discourses. Many features of real discourses are missing here: real coherence, in the sense that the individual discourse units build on each other; cohesion, in the sense that discourse-new and discourse-old is properly signalled. But for the purposes here, this can be seen as a feature, as it removes all cues to this task other than semantic ones. To decide whether another mention of an entity type co-refers to a previous one, here a model really must reason about whether the event it occurs in is compatible, what number of entities of this type are likely to be found in a scene of this kind, and so on. This argues that this tasks is still interesting from a semantic perspective, even if a model trained on this data would not directly be transferable to real, natural text. (As a final note, however, it would be possible to annotate the image paragraphs for co-reference and test the model on them, or even train on that data.)"
   ]
  }
 ],
 "metadata": {
  "author": "Natural Language Semantics with Pictures: Some Language & Vision Datasets and Potential Uses for Computational Semantics",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "../Common/joint.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
