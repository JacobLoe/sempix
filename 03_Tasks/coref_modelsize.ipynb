{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*David Schlangen, 2019-03-24*\n",
    "\n",
    "# Task: Resolving Co-Reference / Predicting Model Size\n",
    "\n",
    "In the section on discourses in the denotations section, we have already briefly mentioned the task of co-reference resolution. If the image (model) is available, co-reference is established exophorically, via the anchoring in the image object. If we take away the image, the task must be tackled via linguistic evidence (and common-sense knowledge about scenes) alone. (It hence becomes an inference / entailment task more than one of denotation computation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import Latex, display\n",
    "\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# Load up config file (needs path; adapt env var if necessary); local imports\n",
    "\n",
    "# load config file, set up paths, make project-specific imports\n",
    "config_path = os.environ.get('VISCONF')\n",
    "if not config_path:\n",
    "    # try default location, if not in environment\n",
    "    default_path_to_config = '../../clp-vision/Config/default.cfg'\n",
    "    if os.path.isfile(default_path_to_config):\n",
    "        config_path = default_path_to_config\n",
    "\n",
    "assert config_path is not None, 'You need to specify the path to the config file via environment variable VISCONF.'        \n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config.read_file(f)\n",
    "\n",
    "corpora_base = config.get('DEFAULT', 'corpora_base')\n",
    "preproc_path = config.get('DSGV-PATHS', 'preproc_path')\n",
    "dsgv_home = config.get('DSGV-PATHS', 'dsgv_home')\n",
    "\n",
    "\n",
    "sys.path.append(dsgv_home + '/Utils')\n",
    "from utils import icorpus_code, plot_labelled_bb, get_image_filename, query_by_id\n",
    "from utils import plot_img_cropped, plot_img_ax, invert_dict, get_a_by_b\n",
    "sys.path.append(dsgv_home + '/WACs/WAC_Utils')\n",
    "from wac_utils import create_word2den, is_relational\n",
    "sys.path.append(dsgv_home + '/Preproc')\n",
    "from sim_preproc import load_imsim, n_most_sim\n",
    "\n",
    "sys.path.append('../Common')\n",
    "from data_utils import load_dfs, plot_rel_by_relid, get_obj_bb, compute_distance_objs\n",
    "from data_utils import get_obj_key, compute_relpos_relargs_row, get_all_predicate\n",
    "from data_utils import compute_distance_relargs_row, get_rel_type, get_rel_instances\n",
    "from data_utils import compute_obj_sizes_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-53bbf9d5de77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m df_names = ['vgregdf', #'vgimgdf', 'vgobjdf', 'vgreldf',\n\u001b[1;32m      6\u001b[0m            ]\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreproc_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# a derived DF, containing only those region descriptions which I was able to resolve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/clp/sempix/Common/data_utils.py\u001b[0m in \u001b[0;36mload_dfs\u001b[0;34m(path, inlist)\u001b[0m\n\u001b[1;32m     20\u001b[0m         df[this_df] = pd.read_json(os.path.join(path, this_df + '.json.gz'),\n\u001b[1;32m     21\u001b[0m                                    \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'frame'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                    compression='gzip')\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    535\u001b[0m             )\n\u001b[1;32m    536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'frame'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             decoded = {str(k): v for k, v in compat.iteritems(\n\u001b[0;32m--> 874\u001b[0;31m                 loads(json, precise_float=self.precise_float))}\n\u001b[0m\u001b[1;32m    875\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_keys_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load up preprocessed DataFrames. Slow!\n",
    "# These DataFrames are the result of pre-processing the original corpus data,\n",
    "# as per dsg-vision/Preprocessing/preproc.py\n",
    "\n",
    "df_names = ['vgregdf', #'vgimgdf', 'vgobjdf', 'vgreldf',\n",
    "           ]\n",
    "df = load_dfs(preproc_path, df_names)\n",
    "\n",
    "# a derived DF, containing only those region descriptions which I was able to resolve\n",
    "df['vgpregdf'] = df['vgregdf'][df['vgregdf']['pphrase'].notnull() & \n",
    "                               (df['vgregdf']['pphrase'] != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-reference resolution is the task of determining whether a referring expression introduces a new entity into the discourse or not. We can create data for this task using the visual genome region annotation. Turning the set of region descriptions into a \"discourse\", we have gold truth information about whether a region description that is  added to the discourse introduces a new entity or talks about one that has previously been introduced.\n",
    "\n",
    "This is what a model would have to predict. The result then is a set of co-reference chains, or entity mentions (in order of ocurrence). From a more semantic point of view, the task entails determining the size of the intended model of the discourse; co-reference between two mentions here means that only one individual constant needs to be introduced into the model. This is how it is displayed below, with the maximal model size being the number of entity-denoting expressions (if we were to create a new individual constant for each), the minimal number being the number of entity-types in the discourse (and assuming that all mentions of the same type co-refer), and the actual size being the one indicated by the object resolution of the descriptions. A perfect resolution of the co-references would lead to that number. \n",
    "\n",
    "(Note that the example here is relies on the provided object identifiers to distinguish objects, but visual genome seems to have insuffiently consolidated on that score. To create a cleaner dataset, to make the judgement whether a new object is introduced or not, a test of overlap (intersection over union) between bounding boxes should be performed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# deep caption with co-reference on object level\n",
    "def extr_disc_ref_pphr(pphr):\n",
    "    discourse_referents = []\n",
    "    for token in pphr.split():\n",
    "        subtoken = token.split('|')\n",
    "        if len(subtoken) > 1:\n",
    "            word = subtoken.pop(0)\n",
    "            id_syn_list = zip(subtoken[::2], subtoken[1::2])\n",
    "            discourse_referents.extend([(int(e[0]), e[1]) for e in id_syn_list])\n",
    "    return discourse_referents\n",
    "\n",
    "def cond_print(instr, show):\n",
    "    if show:\n",
    "        print(instr)\n",
    "\n",
    "def model_size_stats(df, image_id, show=False):\n",
    "    all_pphr = df[df['image_id'] == image_id][['phrase', 'pphrase']].values.tolist()\n",
    "    all_discourse_referents = []\n",
    "    all_types = set()\n",
    "    n_mentions = 0\n",
    "    for this_phr, this_pphr in all_pphr:\n",
    "        cond_print(this_phr, show)\n",
    "        this_disc_refs = extr_disc_ref_pphr(this_pphr)\n",
    "        n_mentions += len(this_disc_refs)\n",
    "        #print '   ', this_disc_refs\n",
    "        #this_disc_refs_ids, this_disc_ref_types = zip(*this_disc_refs)\n",
    "        for disc_ref, ref_type in this_disc_refs:\n",
    "            if disc_ref in all_discourse_referents:\n",
    "                cond_print(\"    OLD: %d (%s)\" % (disc_ref, ref_type), show)\n",
    "            else:\n",
    "                cond_print(\"    NEW: %d (%s)\" % (disc_ref, ref_type), show)\n",
    "                all_discourse_referents.append(disc_ref)\n",
    "                if ref_type in all_types:\n",
    "                    cond_print('    old type, new instance: %d %s' % (disc_ref, ref_type), show)\n",
    "                all_types.add(ref_type)\n",
    "        cond_print('-' * 10, show)\n",
    "    cond_print('max model size: %d || min model size: %d || actual model size: %d'\\\n",
    "                    % (n_mentions, len(all_types), len(all_discourse_referents)), show)\n",
    "    return n_mentions, len(all_types), len(all_discourse_referents)\n",
    "\n",
    "n_egs = 3\n",
    "\n",
    "for _ in range(n_egs):\n",
    "    print(\"=\" * 70)\n",
    "    ii = df['vgpregdf'].sample()['image_id'].values[0]\n",
    "    model_size_stats(df['vgpregdf'], ii, show=True)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the examples here show, these aren't particularly nice discourses. Many features of real discourses are missing here: real coherence, in the sense that the individual discourse units build on each other; cohesion, in the sense that discourse-new and discourse-old is properly signalled. But for the purposes here, this can be seen as a feature, as it removes all cues to this task other than semantic ones. To decide whether another mention of an entity type co-refers to a previous one, here a model really must reason about whether the event it occurs in is compatible, what number of entities of this type are likely to be found in a scene of this kind, and so on. This argues that this tasks is still interesting from a semantic perspective, even if a model trained on this data would not directly be transferable to real, natural text. (As a final note, however, it would be possible to annotate the image paragraphs for co-reference and test the model on them, or even train on that data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "Natural Language Semantics with Pictures: Some Language & Vision Datasets and Potential Uses for Computational Semantics",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "../Common/joint.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
