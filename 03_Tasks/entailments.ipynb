{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*David Schlangen, 2019-03-24*\n",
    "\n",
    "# Task: Predicting Entailments\n",
    "\n",
    "This notebook gives an overview of tasks that make use of images as implicit link between utterances. What follows from the fact that two expressions were provided for the same image (object)? \n",
    "\n",
    "In the perspective established above, where we see images as models of sentences, the question would be \"what follows from being true in the same model\"? In logics, the answer would be \"not much\", as there models are supposed to be stand-ins for the world as a whole (or rather, for one possible world among infinitely many), and many things can be true at the same time, without there being a logical connection between these facts. \n",
    "\n",
    "But our models are perhaps better described as *situations* with some internal coherence stemming from the fact that they are individual slices of the world. (Hence, *situation semantics* \\cite{barwiseperry:sitatt} might have been the more appropriate formalisation to use in the background, but it is a bit more involved than first-order logic, which I used here.) \n",
    "\n",
    "We can thus reformulate our question to: \"what follows from being true in the same situation?\", and investigate whether we can derive a notion of *situational entailment*. This we do in this notebook. The strategy will be to go through the various possible combinations of anchors (image objects) and expression types (referring expressions, captions, etc.) and to inspect the resulting expression pairs. We will also create \"negative\" examples of expressions that might have been taken from the same situation, but weren't. If there is any regularity to the phenomenon, a model of it should be able to distinguish between same-situation pairs and different-situation ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the literature, the relation of interest here is typically called *entailment* or *implication*, which is a more general relation than the *logical* entailment studied in formal semantics. In the most general formulation, a sentence  (lets call it the *hypothesis*) is *implied* by another sentence (or set of sentences; let's call this the *premise*), if accepting the premise makes one (more likely to) accept the hypothesis as well \\cite{chierchi:meaning}. (This is also how later the influential \"recognising textual entailment\" challenge \\cite{Dagan:rte} introduced the relation.)\n",
    "\n",
    "This could be called a pragmatic view on the relation, as it revolves around *accepting* a statement. In formal semantics, one abstracts away from this to yield a universally valid relation (independent of what anyone may or may not choose to accept). Interestingly, there are typically two ways in which the relation can then be explicated. Going the semantic route (and then typically calling the relation *semantic consequence* and using $\\models$ as relation symbol), the notion of truth  as introduced above is harnessed, and the definition becomes \"all models that make the premises true also make the hypothesis true\". Going the syntactic route (and then typically calling the relation *syntactic consequence* and using $\\vdash$ as symbol), the relation is assumed to hold if there is a sequence of applications of syntactic rules that transform the premises into the hypothesis. As the rules are seen as truth-preserving, the idea is that both paths, semantic and syntactic, actually describe the same relation (that is, cover the same pairs of premises and hypotheses). This is the case for some logics, but not all. \n",
    "\n",
    "An interesting task could be to try to set up both \"paths\" (via models / truth and via syntactic transformations) for the tasks described here.\n",
    "\n",
    "Before we launch into the investigation of the data, a few words on related work. The influential \"recognising textual entailments\" challenge was already mentioned above. Under the name \"natural language inference\", the task has recently seen enormous renewed interest. Interestingly, the paper starting this revival, \\cite{snli:emnlp2015}, used image captions as starting point. However, instead of making use of the linked image, they only used the caption as trigger, and asked annotators to *imagine* what must, can, or cannot also be true about the described situation. We use the image to skip the imagination part, having ground truth about whether the situation that makes the hypothesis true is the same or not. But, as we will see, this comes at the cost of what perhaps is noisier data. (We have put some of this data to crowdworkers to let them judge similarity; the experiment has been published in \\cite{schlangen:iwcs19} and is documented in another notebook here.)\n",
    "\n",
    "\\cite{youngetal:flickr30k} pioneered the idea of making use of the image / expression relation, defining a notion of *approximate entailment* and testing it via (sets) of images and partially constructed captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, one might ask why this is an important notion to model. Here the answer would be that being able to recognise entailment (or weaker forms of it) is crucial for being able to understand discourses, as they are structured by relations between their constituent expressions. While the presentation below will mostly be organised by the types of expressions that are related (where we extend the discussion to relations between expressions of types other than sentence as well), we will also point out where such relations might be found in real discourses, and what the use of being able to recognise them would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import random\n",
    "from textwrap import fill\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Latex, display\n",
    "\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# Load up config file (needs path; adapt env var if necessary); local imports\n",
    "\n",
    "# load config file, set up paths, make project-specific imports\n",
    "config_path = os.environ.get('VISCONF')\n",
    "if not config_path:\n",
    "    # try default location, if not in environment\n",
    "    default_path_to_config = '../../clp-vision/Config/default.cfg'\n",
    "    if os.path.isfile(default_path_to_config):\n",
    "        config_path = default_path_to_config\n",
    "\n",
    "assert config_path is not None, 'You need to specify the path to the config file via environment variable VISCONF.'        \n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config.read_file(f)\n",
    "\n",
    "corpora_base = config.get('DEFAULT', 'corpora_base')\n",
    "preproc_path = config.get('DSGV-PATHS', 'preproc_path')\n",
    "dsgv_home = config.get('DSGV-PATHS', 'dsgv_home')\n",
    "\n",
    "\n",
    "sys.path.append(dsgv_home + '/Utils')\n",
    "from utils import icorpus_code, plot_labelled_bb, get_image_filename, query_by_id\n",
    "from utils import plot_img_cropped, plot_img_ax, invert_dict, get_a_by_b\n",
    "sys.path.append(dsgv_home + '/WACs/WAC_Utils')\n",
    "from wac_utils import create_word2den, is_relational\n",
    "sys.path.append(dsgv_home + '/Preproc')\n",
    "from sim_preproc import load_imsim, n_most_sim\n",
    "\n",
    "sys.path.append('../Common')\n",
    "from data_utils import load_dfs, plot_rel_by_relid, get_obj_bb, compute_distance_objs\n",
    "from data_utils import get_obj_key, compute_relpos_relargs_row, get_all_predicate\n",
    "from data_utils import compute_distance_relargs_row, get_rel_type, get_rel_instances\n",
    "from data_utils import compute_obj_sizes_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# Load up preprocessed DataFrames. Slow!\n",
    "# These DataFrames are the result of pre-processing the original corpus data,\n",
    "# as per dsg-vision/Preprocessing/preproc.py\n",
    "\n",
    "df_names = [#'saiapr_bbdf', 'saiapr_refdf',\n",
    "            'mscoco_bbdf', 'refcoco_refdf', 'refcocoplus_refdf', 'grex_refdf',\n",
    "            'vgregdf', 'vgimgdf', 'vgobjdf', 'vgreldf',\n",
    "            'vgpardf', 'cococapdf']\n",
    "            # 'flickr_bbdf', 'flickr_capdf', 'flickr_objdf']\n",
    "df = load_dfs(preproc_path, df_names)\n",
    "\n",
    "# a derived DF, containing only those region descriptions which I was able to resolve\n",
    "df['vgpregdf'] = df['vgregdf'][df['vgregdf']['pphrase'].notnull() & \n",
    "                               (df['vgregdf']['pphrase'] != '')]\n",
    "\n",
    "# load up pre-computed similarities\n",
    "coco_sem_sim, coco_sem_map = load_imsim(os.path.join(preproc_path, 'mscoco_sim.npz'))\n",
    "visg_sem_sim, visg_sem_map = load_imsim(os.path.join(preproc_path, 'visgen_sim.npz'))\n",
    "coco_id2semsim = invert_dict(coco_sem_map)\n",
    "visg_id2semsim = invert_dict(visg_sem_map)\n",
    "\n",
    "coco_vis_sim, coco_vis_map = load_imsim(os.path.join(preproc_path, 'mscoco_vis_sim.npz'))\n",
    "visg_vis_sim, visg_vis_map = load_imsim(os.path.join(preproc_path, 'visgen_vis_sim.npz'))\n",
    "coco_id2vissim = invert_dict(coco_vis_map)\n",
    "visg_id2vissim = invert_dict(visg_vis_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fully delve into this, however, we first look how the data might be used to learn representations for word meanings that might support the task of inferring entailment relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Word Representations from Referential Uses\n",
    "\n",
    "There is a long tradition of work on word meaning where it is modelled not via denotations (as in the *denotations* part of this notebook), but rather via contexts of use, as recorded in corpora (see survey in \\cite{turney-pantel:10}), and where the central notion for which it is put to work is not *truth* relative to a model, but rather the (somewhat vaguer) notion of semantic *similarity*. This tradition has recently been refreshed by the advent of more powerful methods for learning these representations from corpora \\cite{Mikolov2013:embeddings}.\n",
    "\n",
    "The context of use that are the basis of these approaches typically are only linguistic contexts as found in text corpora. The image corpora discussed here open up the possiblitiy to structure the context further, by the referential uses that were made of the expressions. For example, we can look at which words occur together in references to an object (*within context* use), and distinguish them from words that don't (*outside* words). (We report results for such an approach in \\cite{zaschla:contground}, showing that for visual referntial similarity, it outperforms purely textually trained representations.)\n",
    "\n",
    "We show some examples here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target word: minivan\n",
      "belonging to same context:\n",
      "    blue, garbage, other, moving, street, mini, van, with, vehicles,\n",
      "beside, truck, middle, the, a, dark, on, car\n",
      "belonging to different context:\n",
      "    shirt, red, front, darker, fourth, from, with, truck, middle, 3, bear,\n",
      "man, right, white, guy, jacket\n",
      "----------------------------------------\n",
      "target word: wearing\n",
      "belonging to same context:\n",
      "    blue, shirt, left, is, of, with, tall, getting, head, the, woman,\n",
      "bald, help, a, man, dressed, who, on, guy\n",
      "belonging to different context:\n",
      "    behind, top, guy, shirt, photo, plate, in, man, and, right, left,\n",
      "that, with, the, on, person, is, completely, gay, first, dog\n"
     ]
    }
   ],
   "source": [
    "# referential context\n",
    "def get_all_refexps(corps, id_triple):\n",
    "    this_refexp = []\n",
    "    for this_corp in coco_ref_corps:\n",
    "        this_refexp.extend(query_by_id(df[this_corp], id_triple, column='refexp'))\n",
    "    return this_refexp\n",
    "\n",
    "coco_ref_corps = ['refcoco_refdf', 'refcocoplus_refdf', 'grex_refdf']\n",
    "\n",
    "min_length_target = 5\n",
    "\n",
    "# Example 1\n",
    "targt_triple = df['refcoco_refdf'].sample()['i_corpus image_id region_id'.split()].values[0]\n",
    "targt_refexp = get_all_refexps(coco_ref_corps, targt_triple)\n",
    "\n",
    "distr_refexps = df['refcoco_refdf'].sample(5)['refexp'].tolist()\n",
    "\n",
    "listA = list(set(' '.join(targt_refexp).split()))\n",
    "listB = list(set(' '.join(distr_refexps).split()))\n",
    "\n",
    "target_word = \"\"\n",
    "while len(target_word) < min_length_target:\n",
    "    target_word_index = random.choice(range(len(listA)))\n",
    "    target_word = listA[target_word_index]\n",
    "_ = listA.pop(target_word_index)\n",
    "\n",
    "print(\"target word:\", target_word)\n",
    "print(\"belonging to same context:\")\n",
    "print('   ', fill(', '.join(listA), 70))\n",
    "print(\"belonging to different context:\")\n",
    "print('   ', fill(', '.join(listB), 70))\n",
    "\n",
    "print('-' * 40)\n",
    "# Example 2\n",
    "targt_triple = df['refcoco_refdf'].sample()['i_corpus image_id region_id'.split()].values[0]\n",
    "targt_refexp = get_all_refexps(coco_ref_corps, targt_triple)\n",
    "\n",
    "distr_refexps = df['refcoco_refdf'].sample(5)['refexp'].tolist()\n",
    "\n",
    "listA = list(set(' '.join(targt_refexp).split()))\n",
    "listB = list(set(' '.join(distr_refexps).split()))\n",
    "\n",
    "target_word = \"\"\n",
    "while len(target_word) < min_length_target:\n",
    "    target_word_index = random.choice(range(len(listA)))\n",
    "    target_word = listA[target_word_index]\n",
    "_ = listA.pop(target_word_index)\n",
    "\n",
    "print(\"target word:\", target_word)\n",
    "print(\"belonging to same context:\")\n",
    "print('   ', fill(', '.join(listA), 70))\n",
    "print(\"belonging to different context:\")\n",
    "print('   ', fill(', '.join(listB), 70))\n",
    "\n",
    "#pos = [tuple(np.random.choice(listA, 2)) for _ in range(10)]\n",
    "#neg = [(np.random.choice(listA), np.random.choice(listB)) for _ in range(10)]\n",
    "#\n",
    "#print \"From same context:\"\n",
    "#for pair in pos:\n",
    "#    print '  {:>10} , {:<10}'.format(pair[0], pair[1])\n",
    "#print \"\"\n",
    "#print \"From different contexts:\"\n",
    "#for pair in neg:\n",
    "#    print '  {:>10} , {:<10}'.format(pair[0], pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this indicates, this method is probably more likely to result in useful representations for nouns and adjectives than for other parts of speech (as can be expected, since the referential function is crucial here).\n",
    "\n",
    "Also, readers that are familiar with this approach will have already seen that this is likely to push terms apart that in other aspects would be seen as semantically very close (e.g., different colours), if they are incompatible on the level of instances of reference (since for example in the corpora it will be rare for the same object to be called both \"black\" and \"red\", or \"large\" and \"small\", or \"man\" and \"woman\"). (For more details, see \\cite{zaschla:contground}.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again summarise properties of the dataset that could be derived from the available data in this way:\n",
    "\n",
    "* **Dataset:** words paired with contexts (words from same referring expression)\n",
    "* **Negative Instances:** words from different referring expression\n",
    "* **Source:** referring expression corpora\n",
    "* **Uses:** learning word representations optimized for *referential* similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicature / Approximate Entailment\n",
    "\n",
    "We extend this approach of using the non-linguistic context to create pairings now to larger expressions, and move from semantic *similarity* to *implication* (or *entailment*, or, to introduce yet another term, to *approximate entailment*, which is how \\cite{youngetal:flickr30k} introduced this task [for captions]; where the qualifier \"approximate\" is added presumably to express the fact that this isn't quite *logical* entailment).\n",
    "\n",
    "The general approach here will be to take positive examples from the set of annotations for the same object (that is, expressions that are related to the same image); for example, two expressions referring to the same image object, or two captions describing the same image. As we will see, this will indeed only yield pairs that are likely to be evaluated similarly in *all* context (more likely in any case than the negative pairs); but generalising this from the fact that this is the case for *one* context is of course potentially fallible. Similarly, an expression taken from another image is only likely not to accidentally apply to the same situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Referring Expressions\n",
    "\n",
    "The following shows some example pairings of referring expressions taken from the same object (premise + p-hyp) or from a different object (premise + n-hyp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  man on right\n",
      "p-hyp:    right person\n",
      "n-hyp:    left bench\n",
      "----------------------------------------\n",
      "premise:  the maroon motorcycle\n",
      "p-hyp:    front bike\n",
      "n-hyp:    black keyboard\n",
      "----------------------------------------\n",
      "premise:  blurry guy farthest left\n",
      "p-hyp:    man in back to the left cut off\n",
      "n-hyp:    person on left\n",
      "----------------------------------------\n",
      "premise:  sofa\n",
      "p-hyp:    couch behind last nights stands\n",
      "n-hyp:    person hat\n",
      "----------------------------------------\n",
      "premise:  the guy with the black jacket big buttons bottom middle\n",
      "p-hyp:    guy front towards right\n",
      "n-hyp:    person in pink pantsblack knees\n",
      "----------------------------------------\n",
      "premise:  equipment the guys are standing on\n",
      "p-hyp:    longbed truck guys are on\n",
      "n-hyp:    white sheep right\n",
      "----------------------------------------\n",
      "premise:  girl standing\n",
      "p-hyp:    right lady\n",
      "n-hyp:    girl on horse\n",
      "----------------------------------------\n",
      "premise:  gray nearest us\n",
      "p-hyp:    in middle\n",
      "n-hyp:    old man\n",
      "----------------------------------------\n",
      "premise:  left girl\n",
      "p-hyp:    woman leftmost\n",
      "n-hyp:    man in leather black jacket\n",
      "----------------------------------------\n",
      "premise:  girl in yellow\n",
      "p-hyp:    girl\n",
      "n-hyp:    groom\n"
     ]
    }
   ],
   "source": [
    "# pairing premise and hypotheses\n",
    "n = 10 # how many to do\n",
    "triples = []\n",
    "\n",
    "this_df = df['refcoco_refdf']\n",
    "\n",
    "for _ in range(n):\n",
    "    # seed image\n",
    "    ic, ii, ri, rexi = this_df.sample()['i_corpus image_id region_id rex_id'.split()].values[0]\n",
    "    premise, phyp =  np.random.choice(query_by_id(this_df, (ic, ii, ri), 'refexp'), 2, replace=False)\n",
    "\n",
    "    # negative hypothesis\n",
    "    nhyp = this_df.sample()['refexp'].values[0]\n",
    "    triples.append((premise, phyp, nhyp))\n",
    "\n",
    "colnames = 'premise p-hyp n-hyp'.split()\n",
    "#pd.DataFrame(triples, columns=colnames)\n",
    "\n",
    "for prem, phyp, nhyp in triples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", prem)\n",
    "    print(\"p-hyp:   \", phyp)\n",
    "    print(\"n-hyp:   \", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, distinguishing between these pairs seems to be a rather easy task, for which attention to lexical items might even be enough. *Explaining* the decision, however, would not be trivial and require knowledge about how speakers are likely to refer, or about what properties are unlikely to co-occur in the same entity. \n",
    "\n",
    "Relating this abstract task to a real(er) discourse task, we can phrase this as recognising whether something can serve as an answer to a clarification request. The positive hypotheses shown here would work as elaborations or reformulations of the description that the premise gives; it is harder to understand the negative hypotheses in that way. \n",
    "\n",
    "Here are some examples put into this kind of context:\n",
    "\n",
    "+ A: black car behind dorks holding signs  \n",
    "  B: what?  \n",
    "  A: the car behind the three people\n",
    "\n",
    "vs \n",
    "\n",
    "+ A: black car behind dorks holdings signs  \n",
    "  B: what?  \n",
    "  A: #closer girl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Captions and \"There is\"-Sentences\n",
    "\n",
    "Where the pairs above combined expressions whose denotations are on the same level, as it were (objects and objects), we can also create unequal pairings. Using a caption as hypothesis (a description of the situation as a whole), we can ask whether it entails the presence of specific objects.\n",
    "\n",
    "Here are some examples of caption paired with an object referred to via a name (slotted into the \"there is __\" frame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "notebook"
    ]
   },
   "outputs": [],
   "source": [
    "# intersecting visual genome and coco captions. Slow-ish.\n",
    "caption_coco_iids = list(set(df['cococapdf']['image_id'].tolist()))\n",
    "# regions for only those image for which we also have coco captions\n",
    "visgencocap_regdf = df['vgregdf'].merge(pd.DataFrame(caption_coco_iids, columns=['coco_id']))\n",
    "# coco_image_ids for images with both caption and region\n",
    "vgcap_coco_iids = list(set(visgencocap_regdf['coco_id'].tolist()))\n",
    "# visgen_image_ids for images with both caption and region\n",
    "vgcap_vg_iids = list(set(visgencocap_regdf['image_id'].tolist()))\n",
    "\n",
    "# map coco_ids to visgen_ids, and back\n",
    "coco2vg = dict(visgencocap_regdf[['coco_id', 'image_id']].values)\n",
    "vg2coco = dict([(v,k) for k,v in coco2vg.items()])\n",
    "\n",
    "df['vgpardf']['coco_image_id'] = df['vgpardf']['image_id'].apply(lambda x: vg2coco.get(x, None))\n",
    "df['cocoparcapdf'] = df['cococapdf'].merge(df['vgpardf'],\n",
    "                                           left_on='image_id', right_on='coco_image_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  Very complicated fake horse being operated by tow people inside it.\n",
      "p-hyp:    there is a sweater\n",
      "n-hyp:    there is a street light\n",
      "----------------------------------------\n",
      "premise:  A man in white shirt doing a kick flip on a skateboard.\n",
      "p-hyp:    there is a skateboard\n",
      "n-hyp:    there is a bald spot\n",
      "----------------------------------------\n",
      "premise:  Don't walk traffic signal with people still walking.\n",
      "p-hyp:    there is a red hand\n",
      "n-hyp:    there is a room\n",
      "----------------------------------------\n",
      "premise:  A man and woman sit together in front of a large piece of cake\n",
      "p-hyp:    there is a cake\n",
      "n-hyp:    there is a screen\n",
      "----------------------------------------\n",
      "premise:  This is a collection of old, discarded refrigerators behind a\n",
      "building.\n",
      "p-hyp:    there is a inside\n",
      "n-hyp:    there is a lid\n",
      "----------------------------------------\n",
      "premise:  A man that is on a snowboard in the air.\n",
      "p-hyp:    there is a trees\n",
      "n-hyp:    there is a sky\n",
      "----------------------------------------\n",
      "premise:  A man with glasses, gray blazer and white shirt\n",
      "p-hyp:    there is a letter a\n",
      "n-hyp:    there is a storefront\n",
      "----------------------------------------\n",
      "premise:  A snowboarder jumps off of a ramp on a cloudy day.\n",
      "p-hyp:    there is a sign\n",
      "n-hyp:    there is a nose\n",
      "----------------------------------------\n",
      "premise:  There is a train with people next to it.\n",
      "p-hyp:    there is a person\n",
      "n-hyp:    there is a engine\n",
      "----------------------------------------\n",
      "premise:  a few men in a field playing frisbee\n",
      "p-hyp:    there is a shoe\n",
      "n-hyp:    there is a board ramp\n"
     ]
    }
   ],
   "source": [
    "# captions and objects (slotted into \"there is __\" frame)\n",
    "tuples = []\n",
    "\n",
    "for _ in range(10):\n",
    "    try:\n",
    "        vgii, cocoii = visgencocap_regdf.sample()['image_id coco_id'.split()].values[0]\n",
    "        prem = df['cococapdf'][df['cococapdf']['image_id'] == cocoii].sample()['caption'].values[0]\n",
    "        phyp = df['vgobjdf'][df['vgobjdf']['image_id'] == vgii].sample()['name'].values[0]\n",
    "        nhyp = df['vgobjdf'].sample()['name'].values[0]\n",
    "        tuples.append((prem, phyp, nhyp))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for prem, phyp, nhyp in tuples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", fill(prem, 70))\n",
    "    print(\"p-hyp:    there is a\", phyp)\n",
    "    print(\"n-hyp:    there is a\", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the same configuration, but with whole region descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "premise:  A big and small ships passing through a bridge.\n",
      "p-hyp:    there is/are (a) words painted on building\n",
      "n-hyp:    there is/are (a) Diced potatoes in a white bowl.\n",
      "========================================\n",
      "premise:  A large crowd of people stand in the middle of a clearing.\n",
      "p-hyp:    there is/are (a) heap of clothes on the ground\n",
      "n-hyp:    there is/are (a) The left eye of a middle aged white man.\n",
      "========================================\n",
      "premise:  A hawk perched on the ledge of a building\n",
      "p-hyp:    there is/are (a) The head of a hawk\n",
      "n-hyp:    there is/are (a) a light on the train\n",
      "========================================\n",
      "premise:  A woman sitting at a table in front of a lot of food.\n",
      "p-hyp:    there is/are (a) plate of muffins\n",
      "n-hyp:    there is/are (a) grill on front of red truck \n",
      "========================================\n",
      "premise:  A woman is dressed as Matilda from Brave.\n",
      "p-hyp:    there is/are (a) Path with small pieces of gravel\n",
      "n-hyp:    there is/are (a) a window on a train \n",
      "========================================\n",
      "premise:  A bull is laying in the open street\n",
      "p-hyp:    there is/are (a) Stores and building on one side of the road\n",
      "n-hyp:    there is/are (a) A TV is sitting on a stand.\n",
      "========================================\n",
      "premise:  A room filled with people sitting around while some stand.\n",
      "p-hyp:    there is/are (a) person sleeping in chair\n",
      "n-hyp:    there is/are (a) horns on the head of a giraffe\n"
     ]
    }
   ],
   "source": [
    "# caption + region description\n",
    "for _ in range(10):\n",
    "    try:\n",
    "        p_caps = []\n",
    "        while len(p_caps) == 0:\n",
    "            coco_ii = np.nan\n",
    "            while np.isnan(coco_ii):\n",
    "                ic, vg_ii, coco_ii = df['vgimgdf'].sample()[['i_corpus', 'image_id', 'coco_id']].values[0]\n",
    "            p_caps = query_by_id(df['cococapdf'], (icorpus_code['mscoco'], coco_ii))\n",
    "        p_cap_ind = random.choice(range(len(p_caps)))\n",
    "        p_cap = p_caps.iloc[p_cap_ind]['caption']\n",
    "        p_row = p_caps.index[p_cap_ind]\n",
    "\n",
    "        p_hyp_regions = query_by_id(df['vgpregdf'], (ic, vg_ii))\n",
    "        p_hyp_regions = p_hyp_regions[~p_hyp_regions['rels'].isnull()]\n",
    "        p_hyp_reg, p_hyp_relids, p_hyp_rels, p_hyp_pphrase = \\\n",
    "            p_hyp_regions.sample()['phrase rel_ids rels pphrase'.split()].values[0]\n",
    "\n",
    "        n_hyp_reg, n_hyp_relids, n_hyp_rels, n_hyp_pphrase = \\\n",
    "            df['vgpregdf'].sample()['phrase rel_ids rels pphrase'.split()].values[0]\n",
    "\n",
    "        print(\"=\" * 40)\n",
    "        print(\"premise: \", fill(p_cap, 70))\n",
    "        print(\"p-hyp:    there is/are (a)\", p_hyp_reg) #, '||', p_hyp_pphrase\n",
    "        print(\"n-hyp:    there is/are (a)\", n_hyp_reg) #, '||', n_hyp_pphrase\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the examples indicate, this configuration seems to create examples that bring out a rather clear version of (approximate, commonsense) entailment (perhaps better called *situational implication*): Does a situation of the type described by the caption typically entail / imply the presence of an object of the type described by the hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Captions\n",
    "\n",
    "Stepping up the complexity of the expressions, here are pairings based on captions and full images. (Note that this, as mentioned above, was also the starting point of the now popular \"natural language inference\" datasets \\cite{snli:emnlp2015}, where the positive pair was taken from COCO captions. The negative pair and the additional \"neutral\" pair, however, were created manually rather than how it is done here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  A little girl holding onto a rope and onto of a wake board.\n",
      "p-hyp:    A little child riding a surfboard being towed by a boat.\n",
      "n-hyp:    A woman takes a horse over a jump at the finish line in an equestrian\n",
      "contest.\n",
      "----------------------------------------\n",
      "premise:  A woman cutting a birthday cake on a table.\n",
      "p-hyp:    A woman sitting at a table with two cakes.\n",
      "n-hyp:    a person riding a motorcycle on a city street\n",
      "----------------------------------------\n",
      "premise:  A pretty red sports car by a building with a clock.\n",
      "p-hyp:    a tower for the farmers market with a clock on each side\n",
      "n-hyp:    A plate with food on it and utensils nearby.\n",
      "----------------------------------------\n",
      "premise:  A man eating a hot dog on a bug in a napkin.\n",
      "p-hyp:    This man bites into a hotdog covered with mustard and ketchup\n",
      "n-hyp:    train operating on a train track with trees in the background\n",
      "----------------------------------------\n",
      "premise:  Three people eating hotdogs standing on the sidewalk.\n",
      "p-hyp:    Three people walking with hot dogs in their hands.\n",
      "n-hyp:    A plate of food has carrots and broccoli.\n",
      "----------------------------------------\n",
      "premise:  Donuts on a conveyor belt in a donuts kitchen.\n",
      "p-hyp:    A conveyor belt that has bagles on it.\n",
      "n-hyp:    A plate with eggs, ham, and grits on it.\n",
      "----------------------------------------\n",
      "premise:  A lit city skyline past a river at night.\n",
      "p-hyp:    a group of large buildings off in the distance.\n",
      "n-hyp:    Bowls of tortellini and broccoli on a countertop\n",
      "----------------------------------------\n",
      "premise:  a man in black and white hitting a tennis ball on a tennis court\n",
      "p-hyp:    A man on a tennis court hitting a tennis ball\n",
      "n-hyp:    Two horses are eating in a field of yellow flowers.\n",
      "----------------------------------------\n",
      "premise:  A bunch of cute small kids in a car with a dog.\n",
      "p-hyp:    Four children sitting in the back of the car with a dog.\n",
      "n-hyp:    Two boats are pictured along a river with a building under\n",
      "construction and a bridge in the background.\n",
      "----------------------------------------\n",
      "premise:  a cat on a wooden structure behind a fence\n",
      "p-hyp:    An orange and white cat sitting on top of a window sill.\n",
      "n-hyp:    A zebra laying on top of a cement ground.\n"
     ]
    }
   ],
   "source": [
    "# pairing premise and hypotheses\n",
    "n = 10 # how many to do\n",
    "triples = []\n",
    "\n",
    "this_df = df['cococapdf']\n",
    "\n",
    "for _ in range(n):\n",
    "    # seed image\n",
    "    ic, ii, rexi = this_df.sample()['i_corpus image_id id'.split()].values[0]\n",
    "    premise, phyp =  np.random.choice(query_by_id(this_df, (ic, ii), 'caption'), 2, replace=False)\n",
    "\n",
    "    # negative hypothesis\n",
    "    nhyp = this_df.sample()['caption'].values[0]\n",
    "    triples.append((premise, phyp, nhyp))\n",
    "\n",
    "#colnames = 'premise p-hyp n-hyp'.split()\n",
    "#pd.DataFrame(triples, columns=colnames)\n",
    "\n",
    "for prem, phyp, nhyp in triples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", fill(prem, 70))\n",
    "    print(\"p-hyp:   \", fill(phyp, 70))\n",
    "    print(\"n-hyp:   \", fill(nhyp, 70))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, the negative hypotheses were selected simply by sampling captions of images other than that from which the premise was taken. Again, to a human reader, the negative hypotheses seem to jump out, and it is not unlikely that a rather shallow model (looking only at semantic similarity of the words and ignoring compositionality) could perform well on this task. But again, *explaining* why the descriptions could be of the same situation (or not), seems like a challenging task, requiring knowledge about event types as well as knowledge about entity types.\n",
    "\n",
    "In any case, we can make the task more challenging, by using our similarity relation between images to select the distractor image from which the negative hypothesis caption is to be taken. The assumption here would be that descriptions of more similar situations should also be more similar, and that to distinguish between them, a deeper understanding of the expression itself is needed.\n",
    "\n",
    "Here are examples using the semantic similarity relation described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  Two girls are sitting next to a man with a large object.\n",
      "p-hyp:    There is a woman and two girls sitting on the bench.\n",
      "n-hyp:    A group of people sitting next to each other near a tree.\n",
      "----------------------------------------\n",
      "premise:  A police man on a motorcycle riding along a parade route.\n",
      "p-hyp:    a man wearing a yellow and blue parka riding a yellow and blue motorcycle\n",
      "n-hyp:    A scooter sits parked in a meat market.\n",
      "----------------------------------------\n",
      "premise:  Two zebras playing in the grassy pasture that's fenced in.\n",
      "p-hyp:    The grassy field is empty except for two zebras.\n",
      "n-hyp:    Zebras playing together in a field with blue skies. \n",
      "----------------------------------------\n",
      "premise:  Two pictures of a woman sitting on a horse\n",
      "p-hyp:    Vintage photographs of a woman on a horse.\n",
      "n-hyp:    A group of men in jockey outfits racing horses.\n",
      "----------------------------------------\n",
      "premise:  A sheep eating hay from a container next to a black sheep.\n",
      "p-hyp:    a sheep eating hay next to a log cabin.\n",
      "n-hyp:    A couple of sheep standing in a metal pen.\n",
      "----------------------------------------\n",
      "premise:  Several people are using their cellphone and standing around at a festival.\n",
      "p-hyp:    two men with no shirt on surrounded by other people\n",
      "n-hyp:    a woman with long hair in a crowd looking at her mobile phone \n",
      "----------------------------------------\n",
      "premise:  A vase is full of flowers on a table.\n",
      "p-hyp:    A bridal table arrangement of white flowers with a hint of red. \n",
      "n-hyp:    Two people with large platters of wine to sample in front of them.\n",
      "----------------------------------------\n",
      "premise:  A person stands at a bar in a restaurant.\n",
      "p-hyp:    The restaurant is empty during this time of day. \n",
      "n-hyp:    A cat standing on a kitchen chair in order to climb on to an armchair\n",
      "----------------------------------------\n",
      "premise:  A group of people are gathered around an outdoor bench. \n",
      "p-hyp:    A group of people gathered together around a park bench.\n",
      "n-hyp:    A man holding an umbrella while standing next to people at a bus stop.\n",
      "----------------------------------------\n",
      "premise:  The herd of cows is walking through the muddy dirt.\n",
      "p-hyp:    Many dairy cows in a field with buildings in the background. \n",
      "n-hyp:    Several brown cows eating grass in an open field\n"
     ]
    }
   ],
   "source": [
    "# pairing premise and hypotheses\n",
    "n = 10 # how many to do\n",
    "tuples = []\n",
    "\n",
    "this_df = df['cococapdf']\n",
    "ic = icorpus_code['mscoco']\n",
    "\n",
    "for _ in range(n):\n",
    "    # seed image\n",
    "    ii = np.random.choice(list(coco_id2semsim.keys()))\n",
    "    premise, phyp =  np.random.choice(query_by_id(this_df, (ic, ii), 'caption'), 2, replace=False)\n",
    "\n",
    "    # negative hypothesis\n",
    "    sim_ids = n_most_sim(coco_sem_sim, coco_sem_map, coco_id2semsim[ii], n=5)\n",
    "    n_ii = np.random.choice(sim_ids[1:])\n",
    "    nhyp = this_df[this_df['image_id'] == n_ii]['caption'].values[0]\n",
    "    \n",
    "    tuples.append((premise, phyp, nhyp))\n",
    "\n",
    "# colnames = 'premise p-hyp n-hyp'.split()\n",
    "# pd.DataFrame(tuples, columns=colnames)\n",
    "\n",
    "for prem, phyp, nhyp in tuples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", prem)\n",
    "    print(\"p-hyp:   \", phyp)\n",
    "    print(\"n-hyp:   \", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same with the visual similarity relation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "premise:  A fluffy white cat lays on a wooden dining table. \n",
      "p-hyp:    A white cat with brown facial markings rests on a wooden kitchen table.\n",
      "n-hyp:    A group of three dogs laying on a kitchen floor.\n",
      "----------------------------------------\n",
      "premise:  A group of people that are cooking in a kitchen.\n",
      "p-hyp:    Many chefs in tall, white hats are preparing meals in a kitchen.\n",
      "n-hyp:    An instructor watches six student chefs cut squash.\n",
      "----------------------------------------\n",
      "premise:  The two young girls are holding stuffed animals.\n",
      "p-hyp:    TWO CHILDREN ARE SMILING WITH STUFF ANIMALS \n",
      "n-hyp:    A group of young people standing around a brown teddy bear.\n",
      "----------------------------------------\n",
      "premise:  some snowboards in bags sitting next to a tree \n",
      "p-hyp:    A skateboard and snowboard sitting on top of luggage with surfboards stacked against them.\n",
      "n-hyp:    A man practices tennis with a young child.\n",
      "----------------------------------------\n",
      "premise:  Chairs and an umbrella sit on a beach.\n",
      "p-hyp:    A red umbrella sitting over lawn chairs on a beach.\n",
      "n-hyp:    A bunch of lawn chairs sitting on top of a beach.\n",
      "----------------------------------------\n",
      "premise:  A bench with a long seat is near the grass and a fence.\n",
      "p-hyp:    A small wooden bench near a metal fence.\n",
      "n-hyp:    Two people sitting on a green bench that overlooks a park and the city.\n",
      "----------------------------------------\n",
      "premise:  A group of young people are having a discussion in front of a huge brush pile.\n",
      "p-hyp:    Four teenage boys stand around talking to each other. \n",
      "n-hyp:    A man and a woman standing near a tree. \n",
      "----------------------------------------\n",
      "premise:  A large truck parked on a city street.\n",
      "p-hyp:    a large semi truck with a man on its roof\n",
      "n-hyp:    a couple of men shake hands in front of a truck\n",
      "----------------------------------------\n",
      "premise:  A group of men and women sitting around the dinner table.\n",
      "p-hyp:    Many people sitting around a table sharing wine.\n",
      "n-hyp:    The children are getting their numbers put on for the race\n",
      "----------------------------------------\n",
      "premise:  Two young people laying on the grass using electronics devices.\n",
      "p-hyp:    Two men are using their laptop and cellphone outside on the grass.\n",
      "n-hyp:    A crowd of young people sitting in a park filled with green grass.\n"
     ]
    }
   ],
   "source": [
    "# pairing premise and hypotheses\n",
    "n = 10 # how many to do\n",
    "tuples = []\n",
    "\n",
    "this_df = df['cococapdf']\n",
    "ic = icorpus_code['mscoco']\n",
    "\n",
    "for _ in range(n):\n",
    "    # seed image\n",
    "    ii = np.random.choice(list(coco_id2semsim.keys()))\n",
    "    premise, phyp =  np.random.choice(query_by_id(this_df, (ic, ii), 'caption'), 2, replace=False)\n",
    "\n",
    "    # negative hypothesis\n",
    "    sim_ids = n_most_sim(coco_vis_sim, coco_vis_map, coco_id2vissim[ii], n=5)\n",
    "    n_ii = np.random.choice(sim_ids[1:])\n",
    "    nhyp = this_df[this_df['image_id'] == n_ii]['caption'].values[0]\n",
    "    \n",
    "    tuples.append((premise, phyp, nhyp))\n",
    "\n",
    "#colnames = 'premise p-hyp n-hyp'.split()\n",
    "#pd.DataFrame(tuples, columns=colnames)\n",
    "\n",
    "for prem, phyp, nhyp in tuples:\n",
    "    print(\"-\" * 40)\n",
    "    print(\"premise: \", prem)\n",
    "    print(\"p-hyp:   \", phyp)\n",
    "    print(\"n-hyp:   \", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these examples illustrate, the similarity might even sometimes be too great, so that the assumption that the negative hypothesis is less likely to also work is broken. It looks like some fine tuning (and testing on human raters) would be needed to find the right degree of (dis)similarity to produce a challenging but promising dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Captions and Paragraphs\n",
    "\n",
    "The expressions do not need to be of the same type; the more important requirement is that they relate to the same type of object. Here we show captions as premise, and image paragraphs as hypotheses (with the negative instances coming from similar images). The hypotheses thus could be seen as elaborations of the situation described by the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "A couple making faces at a wine tasting bar\n",
      "----------------------------------------\n",
      "A woman that make a silly face holds a wine of glass. The woman has blonde hair and blue eyes. The man has black shirt with white designs. A nice picture is above the door. A young man next a door. Two lights hang from the ceiling.\n",
      "----------------------------------------\n",
      "Two little kids are looking at the camera. One of the girls is smiling and the other girl has her thumb in her mouth. They both appear to be right next to a window of a moving home.\n",
      "========================================\n",
      "Some very big cute elephants in a lush green area.\n",
      "----------------------------------------\n",
      "This photo is of two elephants in a forested area. There are lots of trees which have many green leaves. The ground also appears to have lots of green grass and vegetation. The elephants are gray. They have large ears and four legs. One of the elephants is facing the other. The tail of the elephant facing away is visible. The elephant in the center of the image appears to be missing a tusk. There is also a rock in the picture. \n",
      "----------------------------------------\n",
      "There are several elephants in the shallow water. Little elephants are in the middle of the big elephants. The other elephants are in the forest. The forest is dry and dusty. The trees in the forest are brown and dry.\n",
      "========================================\n",
      "Two benches sitting on a sidewalk next to the ocean.\n",
      "----------------------------------------\n",
      "Two white wooden park benches are situated next to each other on a cement walk that looks out on the ocean. There is a dark metal railing in front of the bench that separates them from the sandy beach. Behind the benches is a small strip of sand and some green beach grasses are planted there. The ocean is very blue with a white crest rolling into the shore. It is a sunny day, and a strong shadow from a structure is cast onto the bench area.\n",
      "----------------------------------------\n",
      "Three little birds sit on a weathered log. It is a sunny day outside. The birds are brown, black, and white, and appear to be sparrows. The birds have black eyes, and a yellow beak. The two birds on the left are looking toward the camera, and the bird on the right is looking away from the camera. There are green leaves in the background, as well as some red berries on the branches.\n",
      "========================================\n",
      "A black cow standing next to a wooden fence\n",
      "----------------------------------------\n",
      "A black cow is standing right against the wooden fence. The cow's eyes are dark brown and the cow's hair is wavy. The cows ear is tagged. Behind the black cow stands a brown cow. Their area is fenced off by wood and metal. The area is thick with rich grass. \n",
      "----------------------------------------\n",
      "Two light brown cows stand on a muddy path.  They have horns on their heads.  There are several cows on the ground behind them.  There is a thatched roofed building in the back ground. The building behind it had a blue tarp on part of the roof.  The roofs of several structures can be seen in the background.\n",
      "========================================\n",
      "a couple of people in the field with a freez be\n",
      "----------------------------------------\n",
      "Two men are playing frisbee in a large green field, surrounded by green trees. The sky is cloudy. There is a fence on one side of the field. Both men have dark brown hair. One of the men is wearing a tan hat, blue shirt and blue shorts. The other man is wearing a orange and blue shirt and black shorts. The frisbee is white.\n",
      "----------------------------------------\n",
      "Two people playing Frisbee in the backyard of a house.  One person is a child and the other is the father.  The Frisbee is in the air and the smaller person is trying to catch it.  The man is wearing an orange shirt and beige shorts.  The kid is wearing blue jeans and a blue shirt.  There is a tall tree in the back of the house. The white house has lots of window.\n"
     ]
    }
   ],
   "source": [
    "# caption, paragraph for same image, paragraph for different by similar image\n",
    "n = 5\n",
    "\n",
    "available_iis_cappar = df['cocoparcapdf']['image_id_x']\n",
    "available_iis_sim = coco_id2semsim.keys()\n",
    "available_iis = set(available_iis_cappar).intersection(available_iis_sim)\n",
    "# len(available_iis)    # Only 1503 available...\n",
    "\n",
    "for _ in range(n):\n",
    "    ii = np.random.choice(list(available_iis))\n",
    "    cap, ppar = df['cocoparcapdf'][df['cocoparcapdf']['image_id_x'] == ii]['caption paragraph'.split()].values[0]\n",
    "    all_sim = n_most_sim(coco_sem_sim, coco_sem_map, coco_id2semsim[ii], n=200)\n",
    "    all_neg = set(available_iis).intersection(all_sim)\n",
    "    nii = np.random.choice(list(all_neg))\n",
    "    npar = df['cocoparcapdf'][df['cocoparcapdf']['image_id_x'] == nii]['paragraph'].values[0]\n",
    "\n",
    "    print('=' * 40)\n",
    "    print(cap)\n",
    "    print('-' * 40)\n",
    "    print(ppar)\n",
    "    print('-' * 40)\n",
    "    print(npar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... between Discursive Scene Descriptions and Follow-Ups\n",
    "\n",
    "Following the same recipe, we can create other, related tasks as well, such as the following: \"Given a sequence of region descriptions from one scene, predict whether an additional single region description comes from the same scene or not.\" \n",
    "\n",
    "This is a variant of the Caption / There is task from above, except that here the premise is not assumed to fully describe the base situation; the particular relation that links the hypothesis to the premise could be called \"continuation\" or \"thematic coherence\". \n",
    "\n",
    "First, with randomly selected distractor scene for the wrong hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "The scene:\n",
      "  washing machine drawer for detergent\n",
      "  Light grey bathroom tiles\n",
      "  this is where the toilet paper is housed\n",
      "  this is a toilet holder\n",
      "  a white toilet bowl\n",
      "  the dryer appears to be empty\n",
      "  a tile on the floor\n",
      "  Stained brown wooden table\n",
      "  this is a toilet\n",
      "Which of the following belongs to the same scene?\n",
      " A: edge of a wood\n",
      " B: Rocks on side of pavement\n",
      "========================================\n",
      "The scene:\n",
      "  tooth of a dog\n",
      "  dog with wet ear\n",
      "  paw of a dog\n",
      "  dog with brown eyes\n",
      "  Water in the background\n",
      "  white and gray ocean waves\n",
      "  A dog's brown fur\n",
      "  The dog is facing the camera\n",
      "  A dog's black and brown face\n",
      "Which of the following belongs to the same scene?\n",
      " A: dog with its mouth open\n",
      " B: woman resting her chin in her hand\n",
      "========================================\n",
      "The scene:\n",
      "  cords for hanging the mirror\n",
      "  the floors have dark stain\n",
      "  a black sheet on the bed\n",
      "  a mirror that is framed\n",
      "  pillow on the bed\n",
      "  pillows under the sheets\n",
      "  an ornate mirror above the bed\n",
      "  the curtain is sheer\n",
      "  the mirror is hanging on the wall\n",
      "Which of the following belongs to the same scene?\n",
      " A: curtain with a floral design\n",
      " B: heavy equipment working in a dirt area\n"
     ]
    }
   ],
   "source": [
    "# deep caption + follow up: plausible or not? Randomly selected neg hyp.\n",
    "\n",
    "n_egs = 3\n",
    "\n",
    "for _ in range(n_egs):\n",
    "    ic, ii = df['vgregdf'].sample()[['i_corpus', 'image_id']].values[0]\n",
    "\n",
    "    prem_set_all = list(set(query_by_id(df['vgregdf'], (ic, ii), 'phrase')))\n",
    "    prem_set = np.random.choice(prem_set_all, min(10, len(prem_set_all)), replace=False)\n",
    "    np.random.shuffle(prem_set)\n",
    "    phyp = prem_set[-1]\n",
    "    prem_set = prem_set[:-1]\n",
    "\n",
    "    nii = df['vgregdf'].sample()['image_id'].values[0]\n",
    "    nhyp = np.random.choice(query_by_id(df['vgregdf'], (ic, nii), 'phrase'))\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(\"The scene:\")\n",
    "    for rg in prem_set:\n",
    "        print(' ', rg)\n",
    "    print(\"Which of the following belongs to the same scene?\")\n",
    "    print(\" A:\", phyp)\n",
    "    print(\" B:\", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the distractor scene selected for similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "The scene:\n",
      "  a man wearing a green jacket\n",
      "  arm holding orange jacket\n",
      "  hands of a person with red sleeves\n",
      "  a man wearing a baseball cap\n",
      "  Man rubbing the back of his head\n",
      "  the floor is a granite pattern\n",
      "  Dufflebag in a large plastic bag\n",
      "  man wearing green jacket\n",
      "  the woman wearing the hoodie\n",
      "Which of the following belongs to the same scene?\n",
      " A: the woman wearing the red shirt\n",
      " B: Man in suit casing a black case\n",
      "========================================\n",
      "The scene:\n",
      "  Black tire on front end of truck.\n",
      "  Red dinosaur on truck.\n",
      "  the wheel of a car\n",
      "  tree trunk behind car\n",
      "  Mirror on a car\n",
      "  Silver pole in a parking lot\n",
      "  Blue passenger side mirror. \n",
      "  Double black parking meter by white truck.\n",
      "  antenna on car\n",
      "Which of the following belongs to the same scene?\n",
      " A: Big blue parked truck.\n",
      " B: a police car in front a building\n",
      "========================================\n",
      "The scene:\n",
      "  Short green shrubbery with dead leaves\n",
      "  green grasses behind bench\n",
      "  brown wooden bench in sitting area\n",
      "  white car in the parking lot\n",
      "  back of metal and wood bench\n",
      "  three benches on sidewalk\n",
      "  green grass on field\n",
      "  orange divider wall behind benches\n",
      "  backless bench in the back\n",
      "Which of the following belongs to the same scene?\n",
      " A: grey building in distance\n",
      " B: the grass are green in color\n"
     ]
    }
   ],
   "source": [
    "# deep caption + follow up: plausible or not? Neg hyp from similar image.\n",
    "ic = icorpus_code['visual_genome']\n",
    "reg_sim_iis = list(set(df['vgregdf']['image_id']).intersection(set(visg_id2semsim.keys())))\n",
    "\n",
    "n_egs = 3\n",
    "\n",
    "for _ in range(n_egs):\n",
    "    \n",
    "    ii = np.random.choice(reg_sim_iis)\n",
    "    # ic, ii = df['vgregdf'].sample()[['i_corpus', 'image_id']].values[0]\n",
    "\n",
    "    prem_set_all = list(set(query_by_id(df['vgregdf'], (ic, ii), 'phrase')))\n",
    "    prem_set = np.random.choice(prem_set_all, min(10, len(prem_set_all)), replace=False)\n",
    "    np.random.shuffle(prem_set)\n",
    "    phyp = prem_set[-1]\n",
    "    prem_set = prem_set[:-1]\n",
    "\n",
    "    nm = n_most_sim(visg_sem_sim, visg_sem_map, visg_id2semsim[ii], n=100)\n",
    "    #nm = n_most_sim(visg_vis_sim, visg_vis_map, visg_id2vissim[ii], n=100)\n",
    "    nm = set(df['vgregdf']['image_id']).intersection(set(nm))\n",
    "    nii = np.random.choice(list(nm))\n",
    "    nhyp = np.random.choice(query_by_id(df['vgregdf'], (ic, nii), 'phrase'))\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(\"The scene:\")\n",
    "    for rg in prem_set:\n",
    "        print(' ', rg)\n",
    "    print(\"Which of the following belongs to the same scene?\")\n",
    "    print(\" A:\", phyp)\n",
    "    print(\" B:\", nhyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise this section, the general recipe here is to use the external grounding of the expressions (in images) to construct pairings of expressions that are semantically closely related, with the second part of the pair in some sense following from the first. (Or not, for which case the fact that the expressions come from different images is utilised.)\n",
    "\n",
    "* **Dataset:** pairs of expressions related via the same image\n",
    "* **Negative Instances:** expressions taken from other images\n",
    "* **Source:** visual genome, COCO; derived\n",
    "* **Uses:** learn to predict whether given pair is semantically related or not; learn *(common sense) entailment* relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Would a Model have to Learn, and What Might it Look Like?\n",
    "\n",
    "Having looked at various data sets that can be created, which all bring out different aspects of the general *implies* relation, we can pause briefly to ask what a model that learns this relation from that data would have to learn.\n",
    "\n",
    "This first thing to note here is that it seems that it can't just be logical rules (or rather, the meaning of logical constants) that is to be learned here. This might be enough for pairs such as \"all girls are coding / the girl on the left is coding\", but as the expressions used here come from actual use contexts (albeit in annotations), such textbook examples are unlikely to occur. In many of the cases, at the very least additional *linguistic* knowledge is required (e.g., to relate \"a woman is reading\" and \"a person is reading\"). But beyond that, in many of the examples it seems to be knowledge that presumably goes beyond linguistic knowledge and into the common sense domain to recognise the relation (e.g., to know that in a situation described as \"The skate boarder is riding the skateboard down a slope.\", it is likely that \"there is a helmet\" is also true).\n",
    "\n",
    "A straightforward modern approach now would be to use a high-capacity model (most likely a neural network) to train a classifier that takes a pair of expressions and  predicts whether the relation holds or not. (And which in that sense learns and defines the relation.) This is indeed the approach typically taken to the \"Natural Language Inference\" task \\cite{snli:emnlp2015}, and with some good success.\n",
    "\n",
    "We just note here that in the settings described here, other approaches also seem possible. We said above that the semantic view on the relation is that it holds in case every model of the premise is also a model of the hypothesis. Given a way to evaluate an expression relative to a model, as sketched in Section [Expressions and Denotations](#Expressions-and-Denotations), this quantification over models could indeed be realised, as quantification over all available images.\n",
    "\n",
    "This would, however, require the availability of a reasonably large set of reference models -- which perhaps means that the approach loses in cognitive plausiblity. (If that is a goal.) An approach that sits somewhere between the direct prediction and the model exemplar checking would be one where a (set of) models is *predicted* from the premise, against which the hypothesis is then evaluated. There are methods in the literature that aim to go from natural language expressions (typically, captions) to images or semi-symbolic representations such as image layouts \\cite{Hong2018}, \\cite{Zhao2018}, which could perhaps be used for this. The advantage of such a model would be that it would be inspectable and hence lending itself to also making *explanations* derivable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[<a id=\"cit-barwiseperry:sitatt\" href=\"#call-barwiseperry:sitatt\">1</a>] Jon Barwise and John Perry, ``_Situations and Attitudes_'',  1983.\n",
    "\n",
    "[<a id=\"cit-chierchi:meaning\" href=\"#call-chierchi:meaning\">2</a>] Gennaro Chierchia and Sally McConnell-Ginet, ``_Meaning and Grammar: An Introduction to Semantics_'',  1990.\n",
    "\n",
    "[<a id=\"cit-Dagan:rte\" href=\"#call-Dagan:rte\">3</a>] I. Dagan, O. Glickman and B. Magnini, ``_The PASCAL Recognising Textual Entailment Challenge_'', Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment,  2006.  [online](http://dx.doi.org/10.1007/11736790_9)\n",
    "\n",
    "[<a id=\"cit-snli:emnlp2015\" href=\"#call-snli:emnlp2015\">4</a>] S.R. Bowman, G. Angeli, C. Potts <em>et al.</em>, ``_A large annotated corpus for learning natural language inference_'', Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),  2015.\n",
    "\n",
    "[<a id=\"cit-schlangen:iwcs19\" href=\"#call-schlangen:iwcs19\">5</a>] D. Schlangen, ``_Natural Language Semantics With Pictures: Some Language & Vision Datasets and Potential Uses for Computational Semantics_'', Proceedings of the International Conference on Computational Semantics (IWCS), May 2019.\n",
    "\n",
    "[<a id=\"cit-youngetal:flickr30k\" href=\"#call-youngetal:flickr30k\">6</a>] Young Peter, Lai Alice, Hodosh Micah <em>et al.</em>, ``_From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions_'', Transactions of the Association for Computational Linguistics, vol. 2, number , pp. ,  2014.\n",
    "\n",
    "[<a id=\"cit-turney-pantel:10\" href=\"#call-turney-pantel:10\">7</a>] D. Peter and Pantel Patrick, ``_From Frequency to Meaning: Vector Space Models of Semantics_'', Journal of Artificial Intelligence Research, vol. 37, number , pp. 141--188,  2010.\n",
    "\n",
    "[<a id=\"cit-Mikolov2013:embeddings\" href=\"#call-Mikolov2013:embeddings\">8</a>] T. Mikolov, K. Chen, G. Corrado <em>et al.</em>, ``_Distributed Representations of Words and Phrases and their Compositionality_'', Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013 (NIPS 2013),  2013.\n",
    "\n",
    "[<a id=\"cit-zaschla:contground\" href=\"#call-zaschla:contground\">9</a>] S. Zarrieß and D. Schlangen, ``_Deriving continous grounded meaning representations from referentially structured multimodal contexts_'', Proceedings of EMNLP 2017 -- Short Papers, September 2017.\n",
    "\n",
    "[<a id=\"cit-Hong2018\" href=\"#call-Hong2018\">10</a>] Hong Seunghoon, Yang Dingdong, Choi Jongwook <em>et al.</em>, ``_Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis_'', ArXiv, vol. , number Figure 1, pp. ,  2018.  [online](http://arxiv.org/abs/1801.05091)\n",
    "\n",
    "[<a id=\"cit-Zhao2018\" href=\"#call-Zhao2018\">11</a>] Zhao Bo, Meng Lili, Yin Weidong <em>et al.</em>, ``_Image Generation from Layout_'', ArXiv, vol. , number , pp. ,  2018.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "author": "Natural Language Semantics with Pictures: Some Language \\&amp; Vision Datasets and Potential Uses for Computational Semantics",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "../Common/joint.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
