@article{Vecchi2017,
abstract = {Sophisticated senator and legislative onion. Whether or not you have ever heard of these things, we all have some intuition that one of them makes much less sense than the other. In this paper, we introduce a large dataset of human judgments about novel adjective-noun phrases. We use these data to test an approach to semantic deviance based on phrase representations derived with compositional distributional semantic methods, that is, methods that derive word meanings from contextual information, and approximate phrase meanings by combining word meanings. We present several simple measures extracted from distributional representations of words and phrases, and we show that they have a significant impact on predicting the acceptability of novel adjective-noun phrases even when a number of alternative measures classically employed in studies of compound processing and bigram plausibility are taken into account. Our results show that the extent to which an attributive adjective alters the distributional representation of the noun is the most significant factor in modeling the distinction between acceptable and deviant phrases. Our study extends current applications of compositional distributional semantic methods to linguistically and cognitively interesting problems, and it offers a new, quantitatively precise approach to the challenge of predicting when humans will find novel linguistic expressions acceptable and when they will not.},
author = {Vecchi, Eva M. and Marelli, Marco and Zamparelli, Roberto and Baroni, Marco},
doi = {10.1111/cogs.12330},
file = {:Users/das/Documents/Mendeley/Vecchi et al/2017/Vecchi et al.{\_}2017.pdf:pdf},
issn = {15516709},
journal = {Cognitive Science},
keywords = {Compositionality,Distributional models,Meaning representation,Semantic deviance,Semantic spaces},
pmid = {26991668},
title = {{Spicy Adjectives and Nominal Donkeys: Capturing Semantic Deviance Using Compositionality in Distributional Spaces}},
year = {2017}
}


@inproceedings{Cornia2019,
abstract = {Current captioning approaches can describe images using black-box architectures whose behavior is hardly controllable and explainable from the exterior. As an image can be described in infinite ways depending on the goal and the context at hand, a higher degree of controllability is needed to apply captioning algorithms in complex scenarios. In this paper, we introduce a novel framework for image captioning which can generate diverse descriptions by allowing both grounding and controllability. Given a control signal in the form of a sequence or set of image regions, we generate the corresponding caption through a recurrent architecture which predicts textual chunks explicitly grounded on regions, following the constraints of the given control. Experiments are conducted on Flickr30k Entities and on COCO Entities, an extended version of COCO in which we add grounding annotations collected in a semi-automatic manner. Results demonstrate that our method achieves state of the art performances on controllable image captioning, in terms of caption quality and diversity. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1811.10652},
author = {Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
booktitle = {CVPR 2019},
doi = {arXiv:1811.10652v1},
eprint = {1811.10652},
file = {:Users/das/Documents/Mendeley/Cornia, Baraldi, Cucchiara/2019/Cornia, Baraldi, Cucchiara{\_}2019.pdf:pdf},
title = {{Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions}},
url = {http://arxiv.org/abs/1811.10652},
year = {2019}
}


@article{Bernardi2016,
abstract = {Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.},
archivePrefix = {arXiv},
arxivId = {1601.03896},
author = {Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli and Keller, Frank and Muscat, Adrian and Plank, Barbara},
eprint = {1601.03896},
file = {:Users/das/Documents/Mendeley/Bernardi et al/2016/Bernardi et al.{\_}2016.pdf:pdf},
journal = {ArXiv},
pages = {1--34},
title = {{Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures}},
url = {http://arxiv.org/abs/1601.03896},
year = {2016}
}


@inproceedings{visdial,
  title={{V}isual {D}ialog},
  author={Abhishek Das and Satwik Kottur and Khushi Gupta and Avi Singh and Deshraj Yadav and Jos\'e M.F. Moura and
    Devi Parikh and Dhruv Batra},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017}
}

@inproceedings{DeVries2016:guesswhat,
abstract = {We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spa-tial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial base-lines of the introduced tasks.},
author = {{De Vries}, Harm and Strub, Florian and Chandar, Sarath and Pietquin, Olivier and Larochelle, Hugo and Courville, Aaron},
booktitle = {CVPR 2017},
file = {:Users/das/Documents/Mendeley/De Vries et al/2017/De Vries et al.{\_}2017(2).pdf:pdf},
mendeley-groups = {Tasks and Games},
title = {{GuessWhat?! Visual object discovery through multi-modal dialogue}},
year = {2017}
}

@inproceedings{ilinykh:inlg18,
  author       = {Ilinykh, Nikolai and Zarrieß, Sina and Schlangen, David},
  booktitle    = {Proceedings of 11th International Conference on Natural Language Generation (INLG 2018)},
  location     = {Tilburg, the Netherlands},
  title        = {{The Task Matters. Comparing Image Captioning and Task-Based Dialogical Image Description}},
  year         = {2018},
}

@inproceedings{schlangen:aix,
  author       = {Schlangen, David and Ilinykh, Nikolai and Zarrieß, Sina},
  booktitle    = {Short Paper Proceedings of the 22nd Workshop on the Semantics and Pragmatics of Dialogue (AixDial / semdial 2018)},
  location     = {Aix-en-Provence, France},
  title        = {{MeetUp! A Task For Modelling Visual Dialogue}},
  year         = {2018},
}

@inproceedings{schlangen:sivl18,
  author       = {Schlangen, David and Ilinykh, Nikolai and Zarrieß, Sina},
  booktitle    = {Proceedings of the 1st Workshop on Shortcomings in Vision and Language (SiVL) at ECCV 2018},
  location     = {Munich},
  title        = {{Visual  Dialogue  Needs  Symmetry,  Goals,  andDynamics:  The  Example  of  the  MeetUp  Task}},
  year         = {2018},
}


@article{frankgoodman:rsa,
abstract = {One of the most astonishing features of human language is its capacity to convey information efficiently in context. Many theories provide informal accounts of communicative inference, yet there have been few successes in making precise, quantitative predictions about pragmatic reasoning. We examined judgments about simple referential communication games, modeling behavior in these games by assuming that speakers attempt to be informative and that listeners use Bayesian inference to recover speakers' intended referents. Our model provides a close, parameter-free fit to human judgments, suggesting that the use of information-theoretic tools to predict pragmatic reasoning may lead to more effective formal models of communication.},
author = {Frank, Michael C and Goodman, Noah D},
doi = {10.1126/science.1218633},
file = {:Users/das/Documents/Mendeley/Frank, Goodman/2012/Frank, Goodman{\_}2012.pdf:pdf},
issn = {1095-9203},
journal = {Science},
keywords = {Bayes Theorem,Communication,Experimental,Games,Humans,Judgment,Language,Models,Probability,Statistical},
month = {may},
number = {6084},
pages = {998},
pmid = {22628647},
title = {{Predicting pragmatic reasoning in language games.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22628647},
volume = {336},
year = {2012}
}


@inproceedings{zarriess:inlg18,
  author       = {Zarrieß, Sina and Schlangen, David},
  booktitle    = {Proceedings of the International Conference on Natural Language Generation (INLG)},
  location     = {Tilburg},
  title        = {{Decoding Strategies for Neural Referring Expression Generation}},
  year         = {2018},
}

@article{Glaser1992,
abstract = {Picture naming has become an important experimental paradigm in cognitive psychology. To name a picture can be considered an elementary process in the use of language. Thus, its chtonometric analysis elucidates cognitive structures and processes that underlie speaking. Essentially, these analyses compare picture nam- ing with reading, picture categorizing, and word categorizing. Furthermore, tech- niques of double stimulation such as the paradigms of priming and of Stroop-like inteeetence are used. In this article, recent results obtained with these methods ate reviewed and discussed with regard to five hypotheses about the cognitive structures that ate involved in picture naming. Beside the older hypotheses of internal coding systems with orzly verbal OF only pictorial format, the hypotheses of an internal dual code with a pictorial and a verbal compozert, of a common abstract code with logogen and pictogen subsystems, and the so-called lexical hypothesis are dis- cussed. The latter postulates two main components: an abstract semantic memory which, nevertheless, also subserves picture processing, and a lexicon that catnoes out the huge amount of word processing without semantic interpretation that is necessary in heating, reading, eaking and writing.},
author = {Glaser, Wilhelm R},
doi = {10.1016/0010-0277(92)90040-O},
file = {:Users/das/Documents/Mendeley/Glaser/1992/Glaser{\_}1992.pdf:pdf},
isbn = {0010-0277 (Print)$\backslash$r0010-0277 (Linking)},
issn = {00100277},
journal = {Cognition},
number = {1},
pages = {61--105},
pmid = {1582161},
title = {{Picture naming}},
volume = {42},
year = {1992}
}


@InProceedings{schlangen:iwcs19,
  author = {David Schlangen},
  title = 	 {Natural Language Semantics With Pictures: Some Language & Vision Datasets and Potential Uses for Computational Semantics},
  year = 	 2019,
  booktitle = {Proceedings of the International Conference on Computational Semantics (IWCS)},
  address = 	 {Gothenburg},
  month = 	 {May}}


@Book{barwiseperry:sitatt,
  author = 	 {Jon Barwise and John Perry},
  title = 	 {Situations and Attitudes},
  publisher = 	 {MIT Press},
  year = 	 1983,
  address = 	 {Cambridge, Mass., USA}}


@inproceedings{Suhr2018,
author = {Suhr, Alane and Zhou, Stephanie and Zhang, Iris and Bai, Huajun and Artzi, Yoav},
title = {{A Corpus for Reasoning About Natural Language Grounded in Photographs}},
booktitle = {Proceedings of NIPS 2018},
address={Montreal, Canada},
year = {2018}
}

@inproceedings{Suhr2017,
abstract = {We present a new visual reasoning lan-guage dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sen-tences. We describe a method of crowd-sourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phe-nomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.},
author = {Suhr, Alane and Lewis, Mike and Yeh, James and Artzi, Yoav},
booktitle = {Proceedings of the 2017 meeting of the Association for Computational Linguistics (ACL 2017)},
file = {:Users/das/Documents/Mendeley/Suhr et al/2017/Suhr et al.{\_}2017.pdf:pdf},
mendeley-groups = {Visual Settings/Visual Reasoning},
title = {{A Corpus of Natural Language for Visual Reasoning}},
url = {http://yoavartzi.com/pub/slya-acl.2017.pdf},
year = {2017}
}


@inproceedings{Andreas2016a,
author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
booktitle = {Proceedings of the 2016 Conference on Computer Vision and Pattern Recognition (CVPR 2016)},
title = {{Neural Module Networks}},
year = {2016}
}


@inproceedings{Reed2016,
archivePrefix = {arXiv},
arxivId = {1605.05395},
author = {Reed, Scott and Akata, Zeynep and Schiele, Bernt and Lee, Honglak},
booktitle = {Proceedings of the International Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.13},
eprint = {1605.05395},
file = {:Users/das/Documents/Mendeley/Reed et al/2016/Reed et al.{\_}2016(2).pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
pmid = {21675331},
title = {{Learning Deep Representations of Fine-grained Visual Descriptions}},
url = {http://arxiv.org/abs/1605.05395},
year = {2016}
}

@techreport{WahCUB_200_2011,
	Title = {{The Caltech-UCSD Birds-200-2011 Dataset}},
	Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	Year = {2011},
	Institution = {California Institute of Technology},
	Number = {CNS-TR-2011-001}
}


@inproceedings{zhou2017scene,
    title={Scene Parsing through ADE20K Dataset},
    author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2017}
}


@article{Hudson2019,
archivePrefix = {arXiv},
arxivId = {1902.09506},
author = {Hudson, Drew A and Manning, Christopher D},
eprint = {1902.09506},
file = {:Users/das/Documents/Mendeley/Hudson, Manning/2019/Hudson, Manning{\_}2019.pdf:pdf},
journal = {ArXiv},
title = {{GQA : a new dataset for compositional question answering over real-world images}},
year = {2019}
}


@article{Hodosh:flickr8k,
author = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
doi = {10.1613/jair.3994},
file = {:Users/das/Documents/Mendeley/Hodosh, Young, Hockenmaier/2013/Hodosh, Young, Hockenmaier{\_}2013.pdf:pdf},
isbn = {9781577357384},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {853--899},
title = {{Framing image description as a ranking task: Data, models and evaluation metrics}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84883394520{\&}partnerID=tZOtx3y1},
volume = {47},
year = {2013}
}


@InProceedings{abzianidze-EtAl:2017:EACLshort,
  author    = {Abzianidze, Lasha  and  Bjerva, Johannes  and  Evang, Kilian  and  Haagsma, Hessel  and  van Noord, Rik  and  Ludmann, Pierre  and  Nguyen, Duc-Duy  and  Bos, Johan},
  title     = {The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  month     = {April},
  year      = {2017},
  address   = {Valencia, Spain},
  publisher = {Association for Computational Linguistics},
  pages     = {242--247},
  url       = {http://www.aclweb.org/anthology/E17-2039}
}

@incollection{Bos2017GMB,
   title     = {The Groningen Meaning Bank},
   author    = {Bos, Johan and Basile, Valerio and Evang, Kilian and Venhuizen, Noortje and Bjerva, Johannes},
   booktitle = {Handbook of Linguistic Annotation},
   editor    = {Ide, Nancy and Pustejovsky, James},
   publisher = {Springer},
   volume    = {2},
   pages     = {463--496},
   year      = {2017}
}


@article{Zhao2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1811.11389v2},
author = {Zhao, Bo and Meng, Lili and Yin, Weidong and Sigal, Leonid},
eprint = {arXiv:1811.11389v2},
file = {:Users/das/Documents/Mendeley/Zhao et al/2018/Zhao et al.{\_}2018.pdf:pdf},
journal = {ArXiv},
title = {{Image Generation from Layout}},
year = {2018}
}


@article{Hong2018,
abstract = {We propose a novel hierarchical approach for text-to-image synthesis by inferring semantic layout. Instead of learning a direct mapping from text to image, our algorithm decomposes the generation process into multiple steps, in which it first constructs a semantic layout from the text by the layout generator and converts the layout to an image by the image generator. The proposed layout generator progressively constructs a semantic layout in a coarse-to-fine manner by generating object bounding boxes and refining each box by estimating object shapes inside the box. The image generator synthesizes an image conditioned on the inferred semantic layout, which provides a useful semantic structure of an image matching with the text description. Our model not only generates semantically more meaningful images, but also allows automatic annotation of generated images and user-controlled generation process by modifying the generated scene layout. We demonstrate the capability of the proposed model on challenging MS-COCO dataset and show that the model can substantially improve the image quality, interpretability of output and semantic alignment to input text over existing approaches.},
archivePrefix = {arXiv},
arxivId = {1801.05091},
author = {Hong, Seunghoon and Yang, Dingdong and Choi, Jongwook and Lee, Honglak},
eprint = {1801.05091},
file = {:Users/das/Documents/Mendeley/Hong et al/2018/Hong et al.{\_}2018.pdf:pdf},
journal = {ArXiv},
keywords = {generation,vision},
mendeley-tags = {generation,vision},
number = {Figure 1},
title = {{Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis}},
url = {http://arxiv.org/abs/1801.05091},
year = {2018}
}


@Book{chomsky:synstruc,
  author = 	 {Chomsky, Noam},
  title = 	 {Syntactic Structures},
  publisher = 	 {Mouton & Co.},
  year = 	 1957}


@article{vgg19,
  author    = {Karen Simonyan and
               Andrew Zisserman},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1409.1556},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.1556},
  archivePrefix = {arXiv},
  eprint    = {1409.1556},
  timestamp = {Mon, 13 Aug 2018 16:46:51 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Cirik2018b,
abstract = {We introduce GroundNet, a new neural network for referring expression recognition – the task of localizing (or grounding) in an image the object referred to by a natural language ex-pression. Our approach to this task is the first to rely on a syntactic analysis of the input referring expression in order to inform the structure of the computation graph. Given a parse tree for an input expression, we explicitly map the syntactic constituents and relationships present in the tree to a com-posed graph of neural modules that defines our architecture for performing localization. This syntax-based approach aids localization of both the target object and auxiliary supporting objects mentioned in the expression. As a result, GroundNet is more interpretable than previous methods: we can (1) deter-mine which phrase of the referring expression points to which object in the image and (2) track how the localization of the target object is determined by the network. We study this prop-erty empirically by introducing a new set of annotations on the GoogleRef dataset to evaluate localization of supporting objects. Our experiments show that GroundNet achieves state-of-the-art accuracy in identifying supporting objects, while maintaining comparable performance in the localization of target objects.},
author = {Cirik, Volkan and Berg-kirkpatrick, Taylor and Morency, Louis-philippe},
booktitle = {AAAI 2018},
file = {:Users/das/Documents/Mendeley/Cirik, Berg-kirkpatrick, Morency/2018/Cirik, Berg-kirkpatrick, Morency{\_}2018.pdf:pdf},
keywords = {grounding,referring expressions,vision},
mendeley-groups = {For-Proposals/WAC-18/RefComp},
mendeley-tags = {grounding,referring expressions,vision},
title = {{Using Syntax to Ground Referring Expressions in Natural Images}},
year = {2018}
}


@incollection{Rosch1978,
abstract = {The papers in this book derive from a 1976 meeting sponsored by the Social Science Research Council to discuss the nature and principles of category formation. Part I contains 3 discussions of real-world categories, assuming that the people creating and using the systems can judge similarity between stimuli, perceive and process the attributes of a stimulus, and learn. Part II contains discussions of these 3 abilities, presenting a new theoretical approach to similarity, in which objects are viewed as collections of features, and similarity as a feature-matching process. This theory predicts (a) basic levels of abstraction where the ratio of common to distinctive features is maximized, and (b) the existence of reference items or category prototypes by which other items are judged. Part III studies concepts of representation which are basic to the previous discussions. (PsycINFO Database Record (c) 2002 APA, all rights reserved)},
address = {Hillsdale, N.J., USA},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Rosch, Eleanor},
booktitle = {Cognition and Categorization},
doi = {10.1016/B978-1-4832-1446-7.50028-5},
editor = {Rosch, Eleanor and Lloyd, Barbara B.},
eprint = {0402594v3},
file = {:Users/das/Documents/Mendeley/Rosch/1978/Rosch{\_}1978.pdf:pdf},
isbn = {1558600132},
issn = {0262621592},
pages = {27--48},
pmid = {1177},
primaryClass = {arXiv:cond-mat},
publisher = {Lawrence Erlbaum},
title = {{Principles of Categorization}},
year = {1978}
}


@Book{chierchi:meaning,
  author = 	 {Gennaro Chierchia and McConnell-Ginet, Sally},
  title = 	 {Meaning and Grammar: An Introduction to Semantics},
  publisher = 	 {MIT Press},
  year = 	 1990,
  address = 	 {Cambridge, MA, USA}}

@article{Cer2018,
abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on TF Hub.},
archivePrefix = {arXiv},
arxivId = {1803.11175},
author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St. and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
doi = {arXiv:1803.11175v2},
eprint = {1803.11175},
file = {:Users/das/Documents/Mendeley/Cer et al/2018/Cer et al.{\_}2018.pdf:pdf},
journal = {ArXiv},
title = {{Universal Sentence Encoder}},
url = {http://arxiv.org/abs/1803.11175},
year = {2018}
}


@Book{marr:vision,
  author = 	 {David Marr},
  title = 	 {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  publisher = 	 {W.H. Freeman},
  year = 	 1982,
  address = 	 {San Francisco, USA}
}


@inproceedings{snli:emnlp2015,
	author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	publisher = {Association for Computational Linguistics},
	title = {A large annotated corpus for learning natural language inference},
	year = {2015}
}


@inproceedings{zaschla:contground,
  author       = {Zarrieß, Sina and Schlangen, David},
  booktitle    = {Proceedings of EMNLP 2017 -- Short Papers},
  location     = {Copenhagen},
  month = {September},
  title        = {{Deriving continous grounded meaning representations from referentially structured multimodal contexts}},
  year         = {2017},
}


@InProceedings{Mikolov2013:embeddings,
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
  booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013 (NIPS 2013)},
  year = 	 2013,
  pages = 	 {3111-3119},
  address = 	 {Lake Tahoe, Nevada, USA}}


@Article{turney-pantel:10,
   author = 	 {Peter D. Turney and Patrick Pantel},
   title = 	 {From Frequency to Meaning: Vector Space Models of Semantics},
   journal = 	 {Journal of Artificial Intelligence Research},
   year = 	 2010,
   volume = 	 37,
   pages = 	 {141--188}}


@inproceedings{Dagan:rte,
 author = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
 title = {The PASCAL Recognising Textual Entailment Challenge},
 booktitle = {Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment},
 series = {MLCW'05},
 year = {2006},
 isbn = {3-540-33427-0, 978-3-540-33427-9},
 location = {Southampton, UK},
 pages = {177--190},
 numpages = {14},
 url = {http://dx.doi.org/10.1007/11736790_9},
 doi = {10.1007/11736790_9},
 acmid = {2100054},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 


@book{asher_2011, place={Cambridge}, title={Lexical Meaning in Context: A Web of Words}, DOI={10.1017/CBO9780511793936}, publisher={Cambridge University Press}, author={Asher, Nicholas}, year={2011}}


@inproceedings{Zarriess2016,
abstract = {{\textcopyright} 2016 Association for Computational Linguistics. Research on generating referring expressions has so far mostly focussed on "oneshot reference", where the aim is to generate a single, discriminating expression. In interactive settings, however, it is not uncommon for reference to be established in "installments", where referring information is offered piecewise until success has been confirmed. We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories. We train a recently introduced model of grounded word meaning on a data set of REs for objects in images and learn to predict semantically appropriate expressions. In a human evaluation, we observe that users are sensitive to inadequate object names - which unfortunately are not unlikely to be generated from low-level visual input. We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words. We enhance a word-based REG with contextaware, referential installments and find that they substantially improve the referential success of the system.},
author = {Zarrie{\ss}, S. and Schlangen, D.},
booktitle = {54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers},
isbn = {9781510827585},
title = {{Easy things first: Installments improve referring expression generation for objects in photographs}},
volume = {1},
year = {2016}
}


@article{KrahmerDeemter:REGsurvey,
author = {Krahmer, Emiel and van Deemter, Kees},
journal = {CL},
number = {1},
title = {{Computational Generation of Referring Expressions : A Survey}},
volume = {38},
year = {2012}
}


@article{Bar2004,
author = {Bar, Moshe},
journal = {Nature Reviews Neuroscience},
pages = {617--629},
title = {{Visual Objects In Context}},
volume = {5},
year = {2004}
}


@incollection{Rosch1978,
abstract = {The papers in this book derive from a 1976 meeting sponsored by the Social Science Research Council to discuss the nature and principles of category formation. Part I contains 3 discussions of real-world categories, assuming that the people creating and using the systems can judge similarity between stimuli, perceive and process the attributes of a stimulus, and learn. Part II contains discussions of these 3 abilities, presenting a new theoretical approach to similarity, in which objects are viewed as collections of features, and similarity as a feature-matching process. This theory predicts (a) basic levels of abstraction where the ratio of common to distinctive features is maximized, and (b) the existence of reference items or category prototypes by which other items are judged. Part III studies concepts of representation which are basic to the previous discussions. (PsycINFO Database Record (c) 2002 APA, all rights reserved)},
address = {Hillsdale, N.J., USA},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Rosch, Eleanor},
booktitle = {Cognition and Categorization},
doi = {10.1016/B978-1-4832-1446-7.50028-5},
editor = {Rosch, Eleanor and Lloyd, Barbara B.},
eprint = {0402594v3},
file = {:Users/das/Documents/Mendeley/Rosch/1978/Rosch{\_}1978.pdf:pdf},
isbn = {1558600132},
issn = {0262621592},
pages = {27--48},
pmid = {1177},
primaryClass = {arXiv:cond-mat},
publisher = {Lawrence Erlbaum},
title = {{Principles of Categorization}},
year = {1978}
}


@InProceedings{VQA2015,
author = {Stanislaw Antol and Aishwarya Agrawal and Jiasen Lu and Margaret Mitchell and Dhruv Batra and C. Lawrence Zitnick and Devi Parikh},
title = {VQA: Visual Question Answering},
booktitle = {International Conference on Computer Vision (ICCV)},
year = {2015},
}


@Book{milljohns:langperc,
  author = 	 {G. Miller and Phillip Johnson-Laird},
  title = 	 {Language and Perception},
  publisher = 	 {Harvard University Press},
  year = 	 1976}


@InCollection{ventraldorsal:whatwhere,
  author = 	 {Leslie G. Ungerleider and Mortimer Mishkin},
  title = 	 {Two cortical visual systems},
  booktitle = 	 {Analysis of visual behaviour},
  publisher = {MIT Press},
  year = 	 1982,
  editor = 	 {Ingle, D.J. and Goodale, M. A. and Mansfield, R.J.W.},
  address = 	 {Cambridge, MA, USA}}


@article{Landau2016,
abstract = {In this article, I revisit Landau and Jackendoff's () paper, "What and where in spatial language and spatial cognition," proposing a friendly amendment and reformulation. The original paper emphasized the distinct geometries that are engaged when objects are represented as members of object kinds (named by count nouns), versus when they are represented as figure and ground in spatial expressions (i.e., play the role of arguments of spatial prepositions). We provided empirical and theoretical arguments for the link between these distinct representations in spatial language and their accompanying nonlinguistic neural representations, emphasizing the "what" and "where" systems of the visual system. In the present paper, I propose a second division of labor between two classes of spatial prepositions in English that appear to be quite distinct. One class includes prepositions such as in and on, whose core meanings engage force-dynamic, functional relationships between objects, with geometry only a marginal player. The second class includes prepositions such as above/below and right/left, whose core meanings engage geometry, with force-dynamic relationships a passing or irrelevant variable. The insight that objects' force-dynamic relationships matter to spatial terms' uses is not new; but thinking of these terms as a distinct set within spatial language has theoretical and empirical consequences that are new. I propose three such consequences, rooted in the fact that geometric knowledge is highly constrained and early-emerging in life, while force-dynamic knowledge of objects and their interactions is relatively unconstrained and needs to be learned piecemeal over a lengthy timeline. First, the two classes will engage different learning problems, with different developmental trajectories for both first and second language learners; second, the classes will naturally lead to different degrees of cross-linguistic variation; and third, they may be rooted in different neural representations.},
author = {Landau, Barbara},
doi = {10.1111/cogs.12410},
file = {:Users/das/Documents/Mendeley/Landau/2017/Landau{\_}2017.pdf:pdf},
isbn = {1551-6709 (Electronic) 0364-0213 (Linking)},
issn = {15516709},
journal = {Cognitive Science},
keywords = {Cross-linguistic studies,Development,Force-dynamics,Geometry,Linguistics,Spatial cognition,Spatial language,Spatial prepositions},
number = {S2},
pmid = {27634685},
title = {{Update on “What” and “Where” in Spatial Language: A New Division of Labor for Spatial Terms}},
volume = {41},
year = {2017}
}


@article{landau_jackendoff_1993,
title={“What” and “where” in spatial language and spatial cognition},
volume={16},
DOI={10.1017/S0140525X00029733},
number={2},
journal={Behavioral and Brain Sciences},
publisher={Cambridge University Press},
author={Landau, Barbara and Jackendoff, Ray},
year={1993},
pages={217–238}}


@Book{fellbaum:wordnet,
  editor = 	 {Christiane Fellbaum},
  title = 	 {WordNet: An Electronic Lexical Database},
  publisher = 	 {MIT Press},
  year = 	 1998,
  address = 	 {Cambridge, USA}
}


@inproceedings{maskrcnn,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2017.322},
eprint = {1703.06870},
file = {:Users/das/Documents/Mendeley/He et al/2017/He et al.{\_}2017(3).pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {0006-291X},
keywords = {segmentation,vision},
mendeley-tags = {segmentation,vision},
pmid = {303902},
title = {{Mask R-CNN}},
url = {http://arxiv.org/abs/1703.06870},
year = {2017}
}



@article{yolov3,
  title={YOLOv3: An Incremental Improvement},
  author={Redmon, Joseph and Farhadi, Ali},
  journal = {arXiv},
  year={2018}
}

@article{KampPartee:prototypes,
abstract = {Osherson and Smith (1981, Cognition, 11, 237-262) discuss a number of problems which arise for a prototype-based account of the meanings of simple and complex concepts. Assuming that concept combination in such a theory is to be analyzed in terms of fuzzy logic, they show that some complex concepts inevitably get assigned the wrong meanings. In the present paper we argue that many of the problems O{\&}S discovered are due to difficulties that are intrinsic to fuzzy set theory, and that most of them disappear when fuzzy logic is replaced by supervaluation theory. However, even after this replacement one of O{\&}S's central problems remains: the theory still predicts that the degree to which an object is an instance of, say, "stripped apple" must be less than or equal to both the degree to which it is an instance of "striped" and the degree to which it is an instance of "apple", but this constraint conflicts with and the degree to which it is an instance of "apple", but this constraint conflicts with O{\&}S's experimental results. The second part of the paper explores ways of solving this and related problems. This leads us to suggest a number of distinctions and principles concerning how prototypicality and other mechanisms interact and which seem important for semantics generally. Prominent among these are (i) the distinction between on the one hand the logical and semantic properties of concepts and on the other the linguistic that between concepts for which the extension is determined by their prototype and concepts for which extension and prototypicality are independent.},
author = {Kamp, H and Partee, B},
doi = {10.1016/0010-0277(94)00659-9},
file = {:Users/das/Documents/Mendeley/Kamp, Partee/1995/Kamp, Partee{\_}1995.pdf:pdf},
issn = {00100277},
journal = {Cognition},
number = {2},
pages = {129--191},
pmid = {8556840},
title = {{Prototype theory and compositionality.}},
volume = {57},
year = {1995}
}



@InProceedings{schlaetal:imagewac,
  author = 	 {David Schlangen and Sina Zarrie{\ss} and Casey Kennington},
  title = 	 {Resolving References to Objects in Photographs using the Words-As-Classifiers Model},
  booktitle = {Proceedings of ACL 2016},
  year = 	 2016,
  month = 	 {August},
  address = 	 {Berlin, Germany}}


@InProceedings{krauseetal:visgenparas,
  author = 	 {Krause, J. and Johnson, J. and Krishna, R. and Fei-Fei, Li},
  title = 	 {A hierarchical approach for generating descriptive image paragraphs},
  booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition},
  year = 	 2017,
  month = 	 {January}}


@article{Spelke2007,
abstract = {Human cognition is founded, in part, on four systems for representing objects, actions, number, and space. It may be based, as well, on a fifth system for representing social partners. Each system has deep roots in human phylogeny and ontogeny, and it guides and shapes the mental lives of adults. Converging research on human infants, non-human primates, children and adults in diverse cultures can aid both understanding of these systems and attempts to overcome their limits.},
author = {Spelke, Elizabeth S. and Kinzler, Katherine D.},
doi = {10.1111/j.1467-7687.2007.00569.x},
file = {:Users/das/Documents/Mendeley/Spelke, Kinzler/2007/Spelke, Kinzler{\_}2007.pdf:pdf},
isbn = {1467-7687},
issn = {1363755X},
journal = {Developmental Science},
number = {1},
pages = {89--96},
pmid = {17181705},
title = {{Core knowledge}},
volume = {10},
year = {2007}
}


@inproceedings{krishnavisualgenome,
  title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and Bernstein, Michael and Fei-Fei, Li},
  year = {2016},
  url = {https://arxiv.org/abs/1602.07332},
}

@InProceedings{flickr30kent,
  author = 	 {Bryan A. Plummer and Liwei Wang and Christopher M. Cervantes and Juan C. Caicedo and Julia Hockenmaier and Svetlana Lazebnik},
  title = 	 {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models},
  booktitle = {Proceedings of ICCV},
  year = 	 2015}

@article{youngetal:flickr30k,
  author = 	 {Peter Young and Alice Lai and Micah Hodosh and Julia Hockenmaier},
  title = 	 {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  journal = {Transactions of the Association for Computational Linguistics},
  year = 	 2014,
  volume = 	 2}


@InProceedings{Maoetal:cocorefexp_Final,
  author    = {Junhua Mao and
               Jonathan Huang and
               Alexander Toshev and
               Oana Camburu and
               Alan L. Yuille and
               Kevin Murphy},
  title     = {Generation and Comprehension of Unambiguous Object Descriptions},
  booktitle = {Proceedings of CVPR 2016},
  year = 	 2016,
  month = 	 {June},
  address = 	 {Las Vegas, USA}}


@inProceedings{yueatal:refcoco,
  author = {Yu, L. and Poirson, P. and Yang, S. and Berg, A.C. and Berg T.L.},
  title = 	 {Modeling Context in Referring Expressions},
  year = 	 2016,
  booktitle = {Computer Vision – ECCV 2016},
  volume = 	 9906,
  series = 	 {Lecture Notes in Computer Science},
  publisher = {Springer}}


@incollection{mscoco,
year={2014},
booktitle={Computer Vision – ECCV 2014},
volume={8693},
title={Microsoft COCO: Common Objects in Context},
publisher={Springer International Publishing},
author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C.Lawrence},
pages={740-755}
}

@article{Escalante2010,
author = {Escalante, Hugo Jair and Hern\'{a}ndez, Carlos a. and Gonzalez, Jesus a. and L\'{o}pez-L\'{o}pez, a. and Montes, Manuel and Morales, Eduardo F. and {Enrique Sucar}, L. and Villase\~{n}or, Luis and Grubinger, Michael},
journal = {Computer Vision and Image Understanding},
number = {4},
pages = {419--428},
publisher = {Elsevier Inc.},
title = {{The segmented and annotated IAPR TC-12 benchmark}},
url = {http://dx.doi.org/10.1016/j.cviu.2009.03.008},
volume = {114},
year = {2010}
}


@inproceedings{Grubinger2006,
author = {Grubinger, Michael and Clough, Paul and M\"{u}ller, Henning and Deselaers, Thomas},
booktitle = {Proceedings of the International Conference on Language Resources and Evaluation (LREC 2006)},
pages = {13--23},
title = {{The IAPR TC-12 benchmark: a new evaluation resource for visual information systems}},
address = {Genoa, Italy},
year = {2006}
}

@inproceedings{Kazemzadeh2014,
author = {Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara L},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)},
pages = {787--798},
title = {{ReferItGame: Referring to Objects in Photographs of Natural Scenes}},
address = {Doha, Qatar},
year = {2014}
}

@Book{davies:cv,
  author = 	 {Davies, E. R.},
  title = 	 {Computer and Machine Vision: Theory, Algorithms, Practicalities},
  publisher = 	 {Elsevier},
  year = 	 2012,
  address = 	 {Amsterdam, Boston, Heidelberg, London},
  edition = 	 {4th}}

